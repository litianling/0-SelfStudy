==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
ERROR: [SIM 211-100] 'csim_design' failed: compilation error(s).
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
ERROR: [SIM 211-100] 'csim_design' failed: compilation error(s).
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
ERROR: [SIM 211-100] 'csim_design' failed: compilation error(s).
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:09 . Memory (MB): peak = 185.371 ; gain = 93.824
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:09 . Memory (MB): peak = 185.371 ; gain = 93.824
INFO: [HLS 200-10] Starting code transformations ...
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.461 ; gain = 93.926
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.461 ; gain = 93.926
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.461 ; gain = 93.926
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:66) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:129) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:131) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:163) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:167) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:169) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:171) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:175) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:177) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:182) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:215) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:216) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:217) automatically.
ERROR: [SYNCHK 200-11] f_b_1/forw_back_LTL.c:51: Constant 'data' has an unsynthesizable type '%struct.Data.0.3.5 = type { [30 x [30 x float]], [28 x [28 x...' (possible cause(s): structure variable cannot be decomposed due to (1) unsupported type conversion; (2) memory copy operation; (3) function pointer used in struct; (4) unsupported pointer comparison).
ERROR: [SYNCHK 200-61] f_b_1/forw_back_LTL.c:99: unsupported memory access on variable 'input_matrix' which is (or contains) an array with unknown size at compile time.
INFO: [SYNCHK 200-10] 2 error(s), 0 warning(s).
ERROR: [HLS 200-70] Synthesizability check failed.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.328 ; gain = 93.773
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.328 ; gain = 93.773
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.328 ; gain = 93.773
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:73) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:136) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:138) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:170) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:174) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:176) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:178) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:182) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:184) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:186) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:189) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:222) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:223) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:224) automatically.
ERROR: [SYNCHK 200-11] f_b_1/forw_back_LTL.c:57: Constant 'data' has an unsynthesizable type '%struct.Data.0.3.5 = type { [30 x [30 x float]], [28 x [28 x...' (possible cause(s): structure variable cannot be decomposed due to (1) unsupported type conversion; (2) memory copy operation; (3) function pointer used in struct; (4) unsupported pointer comparison).
ERROR: [SYNCHK 200-61] f_b_1/forw_back_LTL.c:106: unsupported memory access on variable 'input_matrix' which is (or contains) an array with unknown size at compile time.
INFO: [SYNCHK 200-10] 2 error(s), 0 warning(s).
ERROR: [HLS 200-70] Synthesizability check failed.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.922 ; gain = 94.316
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.922 ; gain = 94.316
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:01 ; elapsed = 00:00:09 . Memory (MB): peak = 185.922 ; gain = 94.316
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:66) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:129) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:131) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:163) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:167) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:169) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:171) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:175) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:177) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:182) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:215) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:216) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:217) automatically.
ERROR: [SYNCHK 200-11] f_b_1/forw_back_LTL.c:51: Constant 'data' has an unsynthesizable type '%struct.Data.0.3.5 = type { [30 x [30 x float]], [28 x [28 x...' (possible cause(s): structure variable cannot be decomposed due to (1) unsupported type conversion; (2) memory copy operation; (3) function pointer used in struct; (4) unsupported pointer comparison).
ERROR: [SYNCHK 200-61] f_b_1/forw_back_LTL.c:99: unsupported memory access on variable 'input_matrix' which is (or contains) an array with unknown size at compile time.
INFO: [SYNCHK 200-10] 2 error(s), 0 warning(s).
ERROR: [HLS 200-70] Synthesizability check failed.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.703 ; gain = 94.145
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.703 ; gain = 94.145
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:08 . Memory (MB): peak = 185.703 ; gain = 94.145
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:66) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:129) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:131) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:163) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:167) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:169) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:171) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:175) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:177) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:182) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:215) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:216) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:217) automatically.
ERROR: [SYNCHK 200-11] f_b_1/forw_back_LTL.c:51: Constant 'data' has an unsynthesizable type '%struct.Data.0.3.5 = type { [30 x [30 x float]], [28 x [28 x...' (possible cause(s): structure variable cannot be decomposed due to (1) unsupported type conversion; (2) memory copy operation; (3) function pointer used in struct; (4) unsupported pointer comparison).
ERROR: [SYNCHK 200-61] f_b_1/forw_back_LTL.c:99: unsupported memory access on variable 'input_matrix' which is (or contains) an array with unknown size at compile time.
INFO: [SYNCHK 200-10] 2 error(s), 0 warning(s).
ERROR: [HLS 200-70] Synthesizability check failed.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:121:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:121:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:33:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:121:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:122:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:122:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:33:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:122:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:123:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:123:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:33:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:123:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:125:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:44:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:125:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:44:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:126:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:51:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:126:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:51:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:126:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:51:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:127:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:60:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:127:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:60:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:128:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:51:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:128:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:51:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:128:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:51:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:129:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:60:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:60:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:51:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:51:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:51:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:67:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:74:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:83:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:67:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:74:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:83:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:67:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:74:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:92:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:92:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.742 ; gain = 94.203
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.742 ; gain = 94.203
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.742 ; gain = 94.203
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:62) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:62) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:125) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:127) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:129) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:159) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:163) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:165) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:167) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:171) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:173) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:175) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:178) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:211) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:212) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:213) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:02 ; elapsed = 00:00:09 . Memory (MB): peak = 185.742 ; gain = 94.203
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:62) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:62) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:125) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:126) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:127) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:128) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:129) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:130) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:159) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:163) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:165) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:167) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:171) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:173) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:175) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:178) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:211) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:212) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:213) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:100:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:99)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:05 ; elapsed = 00:00:12 . Memory (MB): peak = 185.742 ; gain = 94.203
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio' (f_b_1/forw_back_LTL.c:111:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:243:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:245:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:244:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:240:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:241:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:242:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:239:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:249:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:255:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:260:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:262:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:261:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:257:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:258:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:259:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'input_matrix' (f_b_1/forw_back_LTL.c:119:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:11 ; elapsed = 00:00:18 . Memory (MB): peak = 304.004 ; gain = 212.465
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 18.09 seconds; current allocated memory: 250.812 MB.
INFO: [HLS 200-434] Only 0 loops out of a total 4 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.132 seconds; current allocated memory: 251.036 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.182 seconds; current allocated memory: 251.219 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.124 seconds; current allocated memory: 251.430 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.189 seconds; current allocated memory: 251.731 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.119 seconds; current allocated memory: 251.944 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.gep.input_matrix'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.386 seconds; current allocated memory: 252.851 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.604 seconds; current allocated memory: 254.044 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.514 seconds; current allocated memory: 254.366 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.127 seconds; current allocated memory: 254.603 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.172 seconds; current allocated memory: 254.703 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.109 seconds; current allocated memory: 254.819 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.138 seconds; current allocated memory: 254.996 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.114 seconds; current allocated memory: 255.168 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.178 seconds; current allocated memory: 255.335 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.126 seconds; current allocated memory: 255.546 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.182 seconds; current allocated memory: 255.787 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.142 seconds; current allocated memory: 256.028 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.189 seconds; current allocated memory: 256.160 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.122 seconds; current allocated memory: 256.326 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.177 seconds; current allocated memory: 256.537 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.142 seconds; current allocated memory: 256.754 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.209 seconds; current allocated memory: 256.945 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.157 seconds; current allocated memory: 257.190 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.179 seconds; current allocated memory: 257.328 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.113 seconds; current allocated memory: 257.449 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.371 seconds; current allocated memory: 258.485 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 260.152 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.fc_out_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.239 seconds; current allocated memory: 261.257 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.088 seconds; current allocated memory: 262.891 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 263.688 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.348 seconds; current allocated memory: 264.283 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.355 seconds; current allocated memory: 264.838 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptruncdEe' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3eOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64jbC' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.598 seconds; current allocated memory: 267.336 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.13 seconds; current allocated memory: 268.221 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.426 seconds; current allocated memory: 268.584 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 268.939 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.358 seconds; current allocated memory: 269.469 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulkbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulkbM': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.397 seconds; current allocated memory: 270.012 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.391 seconds; current allocated memory: 270.422 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.359 seconds; current allocated memory: 270.983 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.413 seconds; current allocated memory: 271.559 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32lbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32lbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 0.468 seconds; current allocated memory: 271.928 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grancg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuvdy' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuvdy': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.713 seconds; current allocated memory: 275.174 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 'ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_kexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_kezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_3_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiKfY' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r' and 'label_r' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.226 seconds; current allocated memory: 279.527 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gmb6_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grancg_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gratde_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dwdI_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_kexdS_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddAem_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddBew_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddCeG_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouDeQ_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouEe0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Ffa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Hfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:34 ; elapsed = 00:00:46 . Memory (MB): peak = 380.836 ; gain = 289.297
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 45.919 seconds; peak allocated memory: 279.527 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
ERROR: [SIM 211-100] 'csim_design' failed: compilation error(s).
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
ERROR: [SIM 211-100] 'csim_design' failed: compilation error(s).
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
ERROR: [HLS 200-70] Compilation errors found: In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:23: error: use of undeclared identifier 'input_matrix'
    memcpy(mnist_data,input_matrix,sizeof(float)*30*30);
                      ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:109:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:109:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:109:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:110:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:110:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:112:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:112:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:113:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:114:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:115:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:115:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:115:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:116:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings and 1 error generated.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:119:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:119:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:33:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:119:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:120:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:120:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:33:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:120:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:121:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:121:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:33:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:121:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:33:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:123:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:44:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:123:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:44:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:124:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:51:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:124:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:51:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:124:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:51:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:125:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:60:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:125:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:60:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:126:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:51:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:126:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:51:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:126:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:51:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:127:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:60:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:60:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:51:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:51:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:51:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:67:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:74:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:83:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:67:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:74:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:83:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:67:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:74:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:92:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:92:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:33:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:110:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.176 ; gain = 93.629
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.176 ; gain = 93.629
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.176 ; gain = 93.629
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:62) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:62) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:123) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:125) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:127) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:157) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:161) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:163) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:165) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:169) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:171) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:173) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:176) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:209) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:210) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:211) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:02 ; elapsed = 00:00:09 . Memory (MB): peak = 185.176 ; gain = 93.629
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:62) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:62) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:123) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:124) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:125) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:126) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:127) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:128) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:157) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:161) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:163) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:165) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:169) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:171) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:173) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:176) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:209) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:210) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:211) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:100:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:99)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:05 ; elapsed = 00:00:12 . Memory (MB): peak = 185.176 ; gain = 93.629
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio' (f_b_1/forw_back_LTL.c:111:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:241:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:243:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:242:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:238:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:239:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:240:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:237:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:247:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:253:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:258:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:260:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:259:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:255:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:256:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:257:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:10 ; elapsed = 00:00:17 . Memory (MB): peak = 303.512 ; gain = 211.965
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 17.544 seconds; current allocated memory: 250.577 MB.
INFO: [HLS 200-434] Only 0 loops out of a total 4 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.148 seconds; current allocated memory: 250.801 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.191 seconds; current allocated memory: 250.984 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.113 seconds; current allocated memory: 251.195 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.175 seconds; current allocated memory: 251.511 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.121 seconds; current allocated memory: 251.724 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.321 seconds; current allocated memory: 252.500 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.359 seconds; current allocated memory: 253.420 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.45 seconds; current allocated memory: 253.726 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.124 seconds; current allocated memory: 253.963 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.163 seconds; current allocated memory: 254.062 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.099 seconds; current allocated memory: 254.179 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.136 seconds; current allocated memory: 254.355 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.119 seconds; current allocated memory: 254.527 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.173 seconds; current allocated memory: 254.695 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.124 seconds; current allocated memory: 254.906 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.187 seconds; current allocated memory: 255.163 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.138 seconds; current allocated memory: 255.403 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.182 seconds; current allocated memory: 255.535 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.114 seconds; current allocated memory: 255.701 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.211 seconds; current allocated memory: 255.896 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.146 seconds; current allocated memory: 256.113 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 256.304 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.141 seconds; current allocated memory: 256.549 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.239 seconds; current allocated memory: 256.688 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.111 seconds; current allocated memory: 256.809 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.362 seconds; current allocated memory: 257.845 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.973 seconds; current allocated memory: 259.475 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.fc_out_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.247 seconds; current allocated memory: 260.633 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.21 seconds; current allocated memory: 262.243 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 0.911 seconds; current allocated memory: 263.077 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.351 seconds; current allocated memory: 263.656 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.348 seconds; current allocated memory: 264.196 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptruncdEe' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3eOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64jbC' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.579 seconds; current allocated memory: 266.415 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.105 seconds; current allocated memory: 267.272 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.382 seconds; current allocated memory: 267.568 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.241 seconds; current allocated memory: 267.923 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.345 seconds; current allocated memory: 268.452 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulkbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulkbM': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.404 seconds; current allocated memory: 269.048 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.378 seconds; current allocated memory: 269.406 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.357 seconds; current allocated memory: 269.967 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.401 seconds; current allocated memory: 270.558 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32lbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32lbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 0.511 seconds; current allocated memory: 270.979 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grancg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuvdy' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuvdy': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.692 seconds; current allocated memory: 274.210 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 'ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_kexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_kezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_3_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiKfY' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r' and 'label_r' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.162 seconds; current allocated memory: 278.380 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gmb6_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grancg_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gratde_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dwdI_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_kexdS_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddAem_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddBew_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddCeG_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouDeQ_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouEe0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Ffa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Hfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:33 ; elapsed = 00:00:45 . Memory (MB): peak = 378.969 ; gain = 287.422
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 44.948 seconds; peak allocated memory: 278.380 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.844 ; gain = 94.258
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:07 . Memory (MB): peak = 185.844 ; gain = 94.258
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.844 ; gain = 94.258
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:02 ; elapsed = 00:00:09 . Memory (MB): peak = 185.844 ; gain = 94.258
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:111) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:113) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:115) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:05 ; elapsed = 00:00:12 . Memory (MB): peak = 185.844 ; gain = 94.258
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio' (f_b_1/forw_back_LTL.c:99:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:207:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:217:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:223:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:228:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:225:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:226:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:227:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:10 ; elapsed = 00:00:17 . Memory (MB): peak = 303.602 ; gain = 212.016
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 17.638 seconds; current allocated memory: 250.592 MB.
INFO: [HLS 200-434] Only 0 loops out of a total 4 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.129 seconds; current allocated memory: 250.816 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.174 seconds; current allocated memory: 250.999 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.114 seconds; current allocated memory: 251.210 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.168 seconds; current allocated memory: 251.511 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.119 seconds; current allocated memory: 251.724 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.334 seconds; current allocated memory: 252.500 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.364 seconds; current allocated memory: 253.420 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.452 seconds; current allocated memory: 253.725 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 253.963 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.164 seconds; current allocated memory: 254.062 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.101 seconds; current allocated memory: 254.179 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.134 seconds; current allocated memory: 254.355 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.117 seconds; current allocated memory: 254.527 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.172 seconds; current allocated memory: 254.694 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.125 seconds; current allocated memory: 254.905 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.182 seconds; current allocated memory: 255.147 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.136 seconds; current allocated memory: 255.387 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.174 seconds; current allocated memory: 255.519 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.116 seconds; current allocated memory: 255.685 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.179 seconds; current allocated memory: 255.896 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.187 seconds; current allocated memory: 256.113 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.187 seconds; current allocated memory: 256.304 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.135 seconds; current allocated memory: 256.549 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.182 seconds; current allocated memory: 256.687 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.165 seconds; current allocated memory: 256.808 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.365 seconds; current allocated memory: 257.844 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.948 seconds; current allocated memory: 259.474 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.fc_out_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.191 seconds; current allocated memory: 260.617 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.172 seconds; current allocated memory: 262.227 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 0.872 seconds; current allocated memory: 263.045 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.358 seconds; current allocated memory: 263.640 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 264.196 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptruncdEe' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3eOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64jbC' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.579 seconds; current allocated memory: 266.399 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.038 seconds; current allocated memory: 267.256 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.429 seconds; current allocated memory: 267.567 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.247 seconds; current allocated memory: 267.922 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.339 seconds; current allocated memory: 268.452 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulkbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulkbM': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.393 seconds; current allocated memory: 269.032 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.387 seconds; current allocated memory: 269.421 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.356 seconds; current allocated memory: 269.966 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.396 seconds; current allocated memory: 270.557 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32lbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32lbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 0.467 seconds; current allocated memory: 270.994 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gmb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grancg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuvdy' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuvdy': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.687 seconds; current allocated memory: 274.225 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 'ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_kexdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_kezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_Ffa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_3_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiKfY' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r' and 'label_r' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.161 seconds; current allocated memory: 278.379 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gmb6_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grancg_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gratde_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dwdI_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_kexdS_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddAem_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddBew_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddCeG_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouDeQ_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouEe0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Ffa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Hfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:33 ; elapsed = 00:00:44 . Memory (MB): peak = 379.273 ; gain = 287.688
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 44.67 seconds; peak allocated memory: 278.379 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.219 ; gain = 93.570
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.219 ; gain = 93.570
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:09 . Memory (MB): peak = 185.219 ; gain = 93.570
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:10 . Memory (MB): peak = 185.219 ; gain = 93.570
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' for pipelining.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:111) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:113) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:115) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:14 . Memory (MB): peak = 185.219 ; gain = 93.570
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.7'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.7' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.7'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.6'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.6' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.6'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.5'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.5' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.5'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio' (f_b_1/forw_back_LTL.c:99:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:207:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:217:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:223:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:228:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:225:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:226:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:227:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:17 ; elapsed = 00:00:25 . Memory (MB): peak = 307.000 ; gain = 215.352
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 25.453 seconds; current allocated memory: 253.721 MB.
INFO: [HLS 200-434] Only 1 loops out of a total 2 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.203 seconds; current allocated memory: 254.124 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.338 seconds; current allocated memory: 254.516 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.226 seconds; current allocated memory: 254.888 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.315 seconds; current allocated memory: 255.216 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.168 seconds; current allocated memory: 255.634 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.325 seconds; current allocated memory: 256.452 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.49 seconds; current allocated memory: 257.564 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.6 seconds; current allocated memory: 258.023 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.148 seconds; current allocated memory: 258.324 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.189 seconds; current allocated memory: 258.440 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 258.563 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.143 seconds; current allocated memory: 258.724 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.127 seconds; current allocated memory: 258.903 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.325 seconds; current allocated memory: 259.158 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.165 seconds; current allocated memory: 259.567 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.416 seconds; current allocated memory: 259.854 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.154 seconds; current allocated memory: 260.154 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.2 seconds; current allocated memory: 260.324 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.128 seconds; current allocated memory: 260.496 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.289 seconds; current allocated memory: 260.807 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 261.203 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.342 seconds; current allocated memory: 261.521 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.173 seconds; current allocated memory: 261.886 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.212 seconds; current allocated memory: 261.989 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.127 seconds; current allocated memory: 262.116 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 263.242 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (10.1173ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i1_68', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (9.13 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.165 seconds; current allocated memory: 265.189 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.fc_out_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.406 seconds; current allocated memory: 266.417 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.218 seconds; current allocated memory: 268.312 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 1.053 seconds; current allocated memory: 269.705 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_5ns_6ns_5ns_10_1_1' to 'forw_back_mac_muldEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.658 seconds; current allocated memory: 270.811 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.601 seconds; current allocated memory: 271.816 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptrunceOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.802 seconds; current allocated memory: 274.228 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_3ns_6ns_3ns_7_1_1' to 'forw_back_mac_mullbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mullbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.116 seconds; current allocated memory: 275.338 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.473 seconds; current allocated memory: 275.731 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.283 seconds; current allocated memory: 276.087 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.457 seconds; current allocated memory: 276.931 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 277.827 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.454 seconds; current allocated memory: 278.313 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.418 seconds; current allocated memory: 279.168 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 280.116 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32ncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32ncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 0.563 seconds; current allocated memory: 280.562 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gravdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuxdS' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuxdS': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.787 seconds; current allocated memory: 283.962 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_kezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_3_0' to 'forw_back_fc_out_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiMgi' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.388 seconds; current allocated memory: 288.494 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gocq_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grarcU_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gravdy_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dyd2_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_kezec_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddCeG_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddDeQ_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddEe0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouFfa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouGfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Hfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Lf8_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:43 ; elapsed = 00:00:57 . Memory (MB): peak = 396.691 ; gain = 305.043
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 57.192 seconds; peak allocated memory: 288.494 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [COSIM 212-47] Using XSIM for RTL simulation.
INFO: [COSIM 212-14] Instrumenting C test bench ...
WARNING: [COSIM 212-369] AXI_master port 'in' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv1' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv2' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv3' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc1' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc2' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc3' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'out' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'lr' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
INFO: [COSIM 212-302] Starting C TB testing ... 
ERROR: [COSIM 212-330] Aborting co-simulation: top function 'forw_back' is not invoked in the test bench.
ERROR: [COSIM 212-5] *** C/RTL co-simulation file generation failed. ***
ERROR: [COSIM 212-4] *** C/RTL co-simulation finished: FAIL ***
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [IMPL 213-8] Exporting RTL as a Vivado IP.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:09 . Memory (MB): peak = 185.848 ; gain = 94.270
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:09 . Memory (MB): peak = 185.848 ; gain = 94.270
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:01 ; elapsed = 00:00:10 . Memory (MB): peak = 185.848 ; gain = 94.270
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:11 . Memory (MB): peak = 185.848 ; gain = 94.270
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' for pipelining.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:111) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:113) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:115) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:15 . Memory (MB): peak = 185.848 ; gain = 94.270
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.7'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.7' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.7'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.6'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.6' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.6'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.5'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.5' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.5'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio' (f_b_1/forw_back_LTL.c:99:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:207:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:217:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:223:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:228:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:225:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:226:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:227:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:17 ; elapsed = 00:00:26 . Memory (MB): peak = 307.074 ; gain = 215.496
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 27.084 seconds; current allocated memory: 253.737 MB.
INFO: [HLS 200-434] Only 1 loops out of a total 2 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.162 seconds; current allocated memory: 254.140 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.352 seconds; current allocated memory: 254.563 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 254.935 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.329 seconds; current allocated memory: 255.233 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.171 seconds; current allocated memory: 255.650 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.345 seconds; current allocated memory: 256.467 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.483 seconds; current allocated memory: 257.579 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.634 seconds; current allocated memory: 258.039 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.169 seconds; current allocated memory: 258.339 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.186 seconds; current allocated memory: 258.455 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.11 seconds; current allocated memory: 258.578 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.153 seconds; current allocated memory: 258.739 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.214 seconds; current allocated memory: 258.918 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.295 seconds; current allocated memory: 259.173 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.198 seconds; current allocated memory: 259.582 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.403 seconds; current allocated memory: 259.869 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.153 seconds; current allocated memory: 260.170 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.202 seconds; current allocated memory: 260.339 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.127 seconds; current allocated memory: 260.512 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.283 seconds; current allocated memory: 260.822 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.182 seconds; current allocated memory: 261.218 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.358 seconds; current allocated memory: 261.520 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.176 seconds; current allocated memory: 261.886 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 261.988 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.136 seconds; current allocated memory: 262.116 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.388 seconds; current allocated memory: 263.273 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (10.1173ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i1_68', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (9.13 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.253 seconds; current allocated memory: 265.204 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.363 seconds; current allocated memory: 266.432 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 268.327 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 1.085 seconds; current allocated memory: 269.704 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_5ns_6ns_5ns_10_1_1' to 'forw_back_mac_muldEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.624 seconds; current allocated memory: 270.809 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.615 seconds; current allocated memory: 271.814 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptrunceOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.823 seconds; current allocated memory: 274.239 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_3ns_6ns_3ns_7_1_1' to 'forw_back_mac_mullbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mullbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.134 seconds; current allocated memory: 275.363 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.443 seconds; current allocated memory: 275.757 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.325 seconds; current allocated memory: 276.113 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.427 seconds; current allocated memory: 276.973 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.678 seconds; current allocated memory: 277.854 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.464 seconds; current allocated memory: 278.340 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.42 seconds; current allocated memory: 279.196 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.641 seconds; current allocated memory: 280.145 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32ncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32ncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 0.569 seconds; current allocated memory: 280.591 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gravdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuxdS' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuxdS': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.841 seconds; current allocated memory: 283.991 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_kezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiLf8' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.381 seconds; current allocated memory: 288.505 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_3_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gocq_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grarcU_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gravdy_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dyd2_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_kezec_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddCeG_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddDeQ_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddEe0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouFfa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouGfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Hfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using block RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:44 ; elapsed = 00:00:59 . Memory (MB): peak = 396.000 ; gain = 304.422
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 59.717 seconds; peak allocated memory: 288.505 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [COSIM 212-47] Using XSIM for RTL simulation.
INFO: [COSIM 212-14] Instrumenting C test bench ...
WARNING: [COSIM 212-369] AXI_master port 'in' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv1' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv2' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv3' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc1' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc2' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc3' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'out' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'lr' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
INFO: [COSIM 212-302] Starting C TB testing ... 
ERROR: [COSIM 212-330] Aborting co-simulation: top function 'forw_back' is not invoked in the test bench.
ERROR: [COSIM 212-5] *** C/RTL co-simulation file generation failed. ***
ERROR: [COSIM 212-4] *** C/RTL co-simulation finished: FAIL ***
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [IMPL 213-8] Exporting RTL as a Vivado IP.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:09 . Memory (MB): peak = 185.258 ; gain = 93.637
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:09 . Memory (MB): peak = 185.258 ; gain = 93.637
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:10 . Memory (MB): peak = 185.258 ; gain = 93.637
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:12 . Memory (MB): peak = 185.258 ; gain = 93.637
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.4' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.5' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.6' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.7' for pipelining.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2': cannot completely unroll a loop with a variable trip count.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply' partially with a factor of 10.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply.1' partially with a factor of 10.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply.2' partially with a factor of 10.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.7' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.7' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:08 ; elapsed = 00:00:18 . Memory (MB): peak = 185.258 ; gain = 93.637
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32:48) in function 'Conv2d.7' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.7'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32:48) in function 'Conv2d.6' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.6'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32:48) in function 'Conv2d.5' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.5'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32:48) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32:48) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32:48) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32:48) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32:48) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio.1' (f_b_1/forw_back_LTL.c:99:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:207:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:217:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:223:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:228:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:225:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:226:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:227:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:20 ; elapsed = 00:00:31 . Memory (MB): peak = 356.684 ; gain = 265.062
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixMultiply.2' to 'MatrixMultiply_2'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixMultiply.1' to 'MatrixMultiply_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixBackPropagatio.1' to 'MatrixBackPropagatio_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 31.856 seconds; current allocated memory: 299.321 MB.
INFO: [HLS 200-434] Only 0 loops out of a total 3 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.16 seconds; current allocated memory: 299.628 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.243 seconds; current allocated memory: 299.885 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.165 seconds; current allocated memory: 300.125 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.275 seconds; current allocated memory: 300.418 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.177 seconds; current allocated memory: 300.676 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.394 seconds; current allocated memory: 301.448 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.347 seconds; current allocated memory: 302.370 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.601 seconds; current allocated memory: 303.166 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.356 seconds; current allocated memory: 304.098 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.601 seconds; current allocated memory: 304.942 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.422 seconds; current allocated memory: 305.884 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.502 seconds; current allocated memory: 306.437 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.946 seconds; current allocated memory: 307.503 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.695 seconds; current allocated memory: 308.018 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.174 seconds; current allocated memory: 308.301 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.213 seconds; current allocated memory: 308.401 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.131 seconds; current allocated memory: 308.560 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.185 seconds; current allocated memory: 308.701 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.173 seconds; current allocated memory: 308.879 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.279 seconds; current allocated memory: 309.058 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.183 seconds; current allocated memory: 309.298 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.278 seconds; current allocated memory: 309.545 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 309.837 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.233 seconds; current allocated memory: 310.007 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.157 seconds; current allocated memory: 310.179 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.231 seconds; current allocated memory: 310.377 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.172 seconds; current allocated memory: 310.640 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.26 seconds; current allocated memory: 310.889 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.189 seconds; current allocated memory: 311.186 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.225 seconds; current allocated memory: 311.325 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.144 seconds; current allocated memory: 311.453 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.597 seconds; current allocated memory: 312.683 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (10.1173ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i7_57', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (9.13 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.279 seconds; current allocated memory: 314.691 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.44 seconds; current allocated memory: 315.907 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.404 seconds; current allocated memory: 317.733 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 1.27 seconds; current allocated memory: 318.815 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_5ns_6ns_5ns_10_1_1' to 'forw_back_mac_muldEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.488 seconds; current allocated memory: 319.484 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.412 seconds; current allocated memory: 320.098 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply_2'.
INFO: [HLS 200-111]  Elapsed time: 0.495 seconds; current allocated memory: 321.916 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply_1'.
INFO: [HLS 200-111]  Elapsed time: 1.112 seconds; current allocated memory: 323.941 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply'.
INFO: [HLS 200-111]  Elapsed time: 1.198 seconds; current allocated memory: 326.037 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptrunceOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 1.336 seconds; current allocated memory: 328.103 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.103 seconds; current allocated memory: 329.051 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.485 seconds; current allocated memory: 329.444 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.327 seconds; current allocated memory: 329.803 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.478 seconds; current allocated memory: 330.368 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mullbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mullbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.538 seconds; current allocated memory: 331.018 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.501 seconds; current allocated memory: 331.458 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.442 seconds; current allocated memory: 332.038 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.539 seconds; current allocated memory: 332.755 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32mb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32mb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio_1'.
INFO: [HLS 200-111]  Elapsed time: 0.556 seconds; current allocated memory: 333.174 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_graocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_graqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_grcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_grasc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gtde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_graudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_8ns_7ns_6ns_13_1_1' to 'forw_back_mac_mulxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_10ns_9ns_8ns_17_1_1' to 'forw_back_mac_mulyd2' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuwdI': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulxdS': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulyd2': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.874 seconds; current allocated memory: 336.790 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_keAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiMgi' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.637 seconds; current allocated memory: 341.264 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_3_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gncg_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_graocq_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_graqcK_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_graudo_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dzec_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_keAem_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddDeQ_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddEe0_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddFfa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouGfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouHfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_IfE_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_KfY_ram (RAM)' using block RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:54 ; elapsed = 00:01:12 . Memory (MB): peak = 460.785 ; gain = 369.164
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 72.101 seconds; peak allocated memory: 341.264 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.461 ; gain = 93.891
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.461 ; gain = 93.891
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:08 . Memory (MB): peak = 185.461 ; gain = 93.891
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:10 . Memory (MB): peak = 185.461 ; gain = 93.891
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'OverturnKernel_label4' (f_b_1/forw_back_LTL.c:84) in function 'OverturnKernel' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.4' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.5' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.6' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.7' for pipelining.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.3': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.2': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.3': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.2': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.2' completely: variable loop bound.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:84) in function 'OverturnKernel' completely with a factor of 3.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.4': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2': cannot completely unroll a loop with a variable trip count.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply' partially with a factor of 10.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply.1' partially with a factor of 10.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply.2' partially with a factor of 10.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.5': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.6': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'Conv2d.7': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'Conv2d.7' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'Conv2d.7' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:08 ; elapsed = 00:00:16 . Memory (MB): peak = 185.461 ; gain = 93.891
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100:10) in function 'MatrixBackPropagation.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'Conv2d.7'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'Conv2d.6'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'Conv2d.5'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio.2' (f_b_1/forw_back_LTL.c:99:45)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:207:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:217:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:223:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:228:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:225:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:226:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:227:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:20 ; elapsed = 00:00:29 . Memory (MB): peak = 357.371 ; gain = 265.801
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixMultiply.2' to 'MatrixMultiply_2'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixMultiply.1' to 'MatrixMultiply_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixBackPropagatio.2' to 'MatrixBackPropagatio_2'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 29.325 seconds; current allocated memory: 299.756 MB.
INFO: [HLS 200-434] Only 0 loops out of a total 3 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.173 seconds; current allocated memory: 300.064 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.249 seconds; current allocated memory: 300.322 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 300.563 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.228 seconds; current allocated memory: 300.872 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.155 seconds; current allocated memory: 301.132 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.388 seconds; current allocated memory: 301.888 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.362 seconds; current allocated memory: 302.847 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.603 seconds; current allocated memory: 303.606 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.353 seconds; current allocated memory: 304.538 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.62 seconds; current allocated memory: 305.398 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.379 seconds; current allocated memory: 306.340 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.538 seconds; current allocated memory: 306.893 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.948 seconds; current allocated memory: 307.944 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.672 seconds; current allocated memory: 308.463 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 308.755 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'OverturnKernel_label4'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_matrix_load_2', f_b_1/forw_back_LTL.c:85) on array 'input_matrix' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_matrix'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.264 seconds; current allocated memory: 308.918 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.144 seconds; current allocated memory: 309.071 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.22 seconds; current allocated memory: 309.263 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.181 seconds; current allocated memory: 309.506 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.255 seconds; current allocated memory: 309.737 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.167 seconds; current allocated memory: 309.979 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.294 seconds; current allocated memory: 310.230 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.202 seconds; current allocated memory: 310.532 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.255 seconds; current allocated memory: 310.701 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.173 seconds; current allocated memory: 310.957 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.244 seconds; current allocated memory: 311.171 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.182 seconds; current allocated memory: 311.435 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.249 seconds; current allocated memory: 311.688 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.188 seconds; current allocated memory: 311.995 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagation_label6_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.256 seconds; current allocated memory: 312.161 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.151 seconds; current allocated memory: 312.335 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagation_label6_L'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_53', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_53', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_53', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_53', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 5, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_53', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 6, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_53', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 7, Depth = 10.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagation_label6_L'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer2_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) of variable 'tmp_i7_55', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180 on array 'fc_hidden_layer2' and 'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer2_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) of variable 'tmp_i7_55', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180 on array 'fc_hidden_layer2' and 'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer2_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) of variable 'tmp_i7_55', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180 on array 'fc_hidden_layer2' and 'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer2_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) of variable 'tmp_i7_55', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180 on array 'fc_hidden_layer2' and 'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 5, Depth = 9.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagation_label6_L'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_58', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_58', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_58', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_58', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 5, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_58', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 6, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_58', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 7, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.192 seconds; current allocated memory: 313.650 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (10.1173ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i7_55', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (9.13 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.389 seconds; current allocated memory: 315.830 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.557 seconds; current allocated memory: 317.127 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.44 seconds; current allocated memory: 318.985 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 1.259 seconds; current allocated memory: 320.104 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_5ns_6ns_5ns_10_1_1' to 'forw_back_mac_muldEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.51 seconds; current allocated memory: 320.773 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.41 seconds; current allocated memory: 321.371 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply_2'.
INFO: [HLS 200-111]  Elapsed time: 0.502 seconds; current allocated memory: 323.189 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply_1'.
INFO: [HLS 200-111]  Elapsed time: 1.107 seconds; current allocated memory: 325.246 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply'.
INFO: [HLS 200-111]  Elapsed time: 1.178 seconds; current allocated memory: 327.331 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptrunceOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 1.385 seconds; current allocated memory: 329.360 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.129 seconds; current allocated memory: 330.315 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.485 seconds; current allocated memory: 330.816 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.375 seconds; current allocated memory: 331.320 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.573 seconds; current allocated memory: 331.955 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mullbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mullbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.559 seconds; current allocated memory: 332.626 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.516 seconds; current allocated memory: 333.190 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.512 seconds; current allocated memory: 333.828 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.538 seconds; current allocated memory: 334.498 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32mb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32mb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio_2'.
INFO: [HLS 200-111]  Elapsed time: 0.591 seconds; current allocated memory: 335.042 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_graocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_graqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_grcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_grasc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gtde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_graudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_8ns_7ns_6ns_13_1_1' to 'forw_back_mac_mulxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_10ns_9ns_8ns_17_1_1' to 'forw_back_mac_mulyd2' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuwdI': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulxdS': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulyd2': 2 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 339.198 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_keAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiMgi' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.8 seconds; current allocated memory: 343.811 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_3_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gncg_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_graocq_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gpcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_graqcK_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grasc4_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_graudo_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dzec_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_keAem_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_keBew_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddDeQ_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddEe0_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddFfa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouGfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouHfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_IfE_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_KfY_ram (RAM)' using block RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:55 ; elapsed = 00:01:11 . Memory (MB): peak = 464.898 ; gain = 373.328
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 71.404 seconds; peak allocated memory: 343.811 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:02 ; elapsed = 00:00:08 . Memory (MB): peak = 186.078 ; gain = 94.551
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:02 ; elapsed = 00:00:08 . Memory (MB): peak = 186.078 ; gain = 94.551
INFO: [HLS 200-10] Starting code transformations ...
INFO: [XFORM 203-603] Inlining function 'Conv2d' into 'backward' (f_b_1/forw_back_LTL.c:173).
INFO: [XFORM 203-603] Inlining function 'Conv2d' into 'backward' (f_b_1/forw_back_LTL.c:170).
INFO: [XFORM 203-603] Inlining function 'Conv2d' into 'backward' (f_b_1/forw_back_LTL.c:164).
INFO: [XFORM 203-603] Inlining function 'Conv2d' into 'backward' (f_b_1/forw_back_LTL.c:161).
INFO: [XFORM 203-603] Inlining function 'Conv2d' into 'backward' (f_b_1/forw_back_LTL.c:155).
INFO: [XFORM 203-603] Inlining function 'Conv2d' into 'forward' (f_b_1/forw_back_LTL.c:108).
INFO: [XFORM 203-603] Inlining function 'Conv2d' into 'forward' (f_b_1/forw_back_LTL.c:107).
INFO: [XFORM 203-603] Inlining function 'Conv2d' into 'forward' (f_b_1/forw_back_LTL.c:106).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:09 . Memory (MB): peak = 186.078 ; gain = 94.551
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:17 ; elapsed = 00:00:25 . Memory (MB): peak = 213.344 ; gain = 121.816
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'forward' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'forward' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'forward' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'backward' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'backward' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'backward' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'backward' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32) in function 'backward' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'OverturnKernel_label4' (f_b_1/forw_back_LTL.c:84) in function 'OverturnKernel' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2' for pipelining.
INFO: [HLS 200-489] Unrolling loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'forward' completely with a factor of 28.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
INFO: [HLS 200-489] Unrolling loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'forward' completely with a factor of 3.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'forward' completely: variable loop bound.
INFO: [HLS 200-489] Unrolling loop 'Conv2d_label1' (f_b_1/forw_back_LTL.c:32) in function 'backward' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'Loop-2.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely with a factor of 3.
INFO: [HLS 200-489] Unrolling loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36) in function 'backward' completely with a factor of 3.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-3.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
INFO: [HLS 200-489] Unrolling loop 'Loop-4.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely with a factor of 3.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-5.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely: variable loop bound.
INFO: [HLS 200-489] Unrolling loop 'Loop-6.1.1' (f_b_1/forw_back_LTL.c:34) in function 'backward' completely with a factor of 3.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.2': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:100) in function 'MatrixBackPropagation.3': cannot completely unroll a loop with a variable trip count.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:84) in function 'OverturnKernel' completely with a factor of 3.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2': cannot completely unroll a loop with a variable trip count.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply' partially with a factor of 10.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply.1' partially with a factor of 10.
INFO: [XFORM 203-501] Unrolling loop 'MatrixMultiply_label0' (f_b_1/forw_back_LTL.c:47) in function 'MatrixMultiply.2' partially with a factor of 10.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:41 ; elapsed = 00:00:51 . Memory (MB): peak = 213.344 ; gain = 121.816
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'forward' : 

more than one sub loop.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'forward' : 

more than one sub loop.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'forward' : 

more than one sub loop.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
WARNING: [XFORM 203-561] 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36:21) in function 'backward' is an infinite loop.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'backward' : 

more than one sub loop.
WARNING: [XFORM 203-561] 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36:21) in function 'backward' is an infinite loop.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Conv2d_label3' (f_b_1/forw_back_LTL.c:32:10) in function 'backward' : 

more than one sub loop.
WARNING: [XFORM 203-561] 'Conv2d_label0' (f_b_1/forw_back_LTL.c:36:21) in function 'backward' is an infinite loop.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagation_label6' (f_b_1/forw_back_LTL.c:100:10) in function 'MatrixBackPropagation.3'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio.2' (f_b_1/forw_back_LTL.c:99:45)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:207:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:217:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:223:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:228:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:225:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:226:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:227:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:01:04 ; elapsed = 00:01:14 . Memory (MB): peak = 346.250 ; gain = 254.723
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'MatrixMultiply.2' to 'MatrixMultiply_2'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixMultiply.1' to 'MatrixMultiply_1'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixBackPropagatio.2' to 'MatrixBackPropagatio_2'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:47): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 74.988 seconds; current allocated memory: 277.800 MB.
INFO: [HLS 200-434] Only 0 loops out of a total 11 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.511 seconds; current allocated memory: 278.729 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.804 seconds; current allocated memory: 279.556 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.514 seconds; current allocated memory: 280.489 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixMultiply' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.776 seconds; current allocated memory: 281.333 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.515 seconds; current allocated memory: 282.275 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 7.323 seconds; current allocated memory: 294.787 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (11.0325ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'fmul' operation ('tmp_0_2', f_b_1/forw_back_LTL.c:36->f_b_1/forw_back_LTL.c:106) (11 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 13.052 seconds; current allocated memory: 323.161 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'OverturnKernel_label4'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_matrix_load_2', f_b_1/forw_back_LTL.c:85) on array 'input_matrix' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_matrix'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 2, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 7.804 seconds; current allocated memory: 324.646 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.271 seconds; current allocated memory: 324.800 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.296 seconds; current allocated memory: 324.991 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.272 seconds; current allocated memory: 325.270 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.335 seconds; current allocated memory: 325.471 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.25 seconds; current allocated memory: 325.727 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagation_label6_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.327 seconds; current allocated memory: 325.856 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.233 seconds; current allocated memory: 326.029 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('conv_out_2_load_73', f_b_1/forw_back_LTL.c:36->f_b_1/forw_back_LTL.c:155) on array 'conv_out_2' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'conv_out_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 8, Depth = 45.
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('conv_out_1_load_79', f_b_1/forw_back_LTL.c:36->f_b_1/forw_back_LTL.c:164) on array 'conv_out_1' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'conv_out_1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 8, Depth = 45.
WARNING: [SCHED 204-65] Unable to satisfy pipeline directive: Loop contains subloop(s) not being unrolled or flattened.
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('mnist_data_load_85', f_b_1/forw_back_LTL.c:36->f_b_1/forw_back_LTL.c:173) on array 'mnist_data' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'mnist_data'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 8, Depth = 45.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagation_label6_L'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_206', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_206', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_206', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_206', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 5, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_206', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 6, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer1_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) of variable 'tmp_i6_206', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179 on array 'fc_hidden_layer1' and 'load' operation ('fc_hidden_layer1_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:179) on array 'fc_hidden_layer1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 7, Depth = 10.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagation_label6_L'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer2_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) of variable 'tmp_i7_208', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180 on array 'fc_hidden_layer2' and 'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer2_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) of variable 'tmp_i7_208', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180 on array 'fc_hidden_layer2' and 'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer2_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) of variable 'tmp_i7_208', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180 on array 'fc_hidden_layer2' and 'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer2_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) of variable 'tmp_i7_208', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180 on array 'fc_hidden_layer2' and 'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 5, Depth = 9.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagation_label6_L'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_211', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_211', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_211', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_211', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 5, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_211', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
WARNING: [SCHED 204-68] The II Violation in module 'backward' (Loop: MatrixBackPropagation_label6_L): Unable to enforce a carried dependence constraint (II = 6, distance = 1, offset = 1)
   between 'store' operation ('fc_hidden_layer3_add_2_write_ln101', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) of variable 'tmp_i8_211', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181 on array 'fc_hidden_layer3' and 'load' operation ('fc_hidden_layer3_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:181) on array 'fc_hidden_layer3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 7, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 6.68 seconds; current allocated memory: 337.353 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (11.0699ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i7_208', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (10.1 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 11.745 seconds; current allocated memory: 361.397 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 8.101 seconds; current allocated memory: 363.405 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 2.786 seconds; current allocated memory: 366.516 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply_2'.
INFO: [HLS 200-111]  Elapsed time: 2.565 seconds; current allocated memory: 370.019 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply_1'.
INFO: [HLS 200-111]  Elapsed time: 1.385 seconds; current allocated memory: 372.136 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixMultiply' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixMultiply'.
INFO: [HLS 200-111]  Elapsed time: 1.372 seconds; current allocated memory: 374.232 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptruncdEe' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3eOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64jbC' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncdEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 8.394 seconds; current allocated memory: 403.394 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 17.063 seconds; current allocated memory: 408.659 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.709 seconds; current allocated memory: 409.173 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 409.787 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32kbM': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio_2'.
INFO: [HLS 200-111]  Elapsed time: 0.83 seconds; current allocated memory: 410.289 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_glbW' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_gramb6' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gncg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_graocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gpcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_graqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_grcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_grasc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gtde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_8ns_7ns_6ns_13_1_1' to 'forw_back_mac_mulvdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_10ns_9ns_8ns_17_1_1' to 'forw_back_mac_mulwdI' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 3 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuudo': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 4 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncdEe': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulvdy': 2 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulwdI': 2 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 5.895 seconds; current allocated memory: 436.980 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_keyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_kezec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_Gfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2Hfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_IfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiKfY' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 17.173 seconds; current allocated memory: 444.751 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_3_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_glbW_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gramb6_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gncg_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_graocq_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_graqcK_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grasc4_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dxdS_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_keyd2_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddBew_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddCeG_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddDeQ_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouEe0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouFfa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Gfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_IfE_ram (RAM)' using block RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:02:56 ; elapsed = 00:03:27 . Memory (MB): peak = 693.613 ; gain = 602.086
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 207.376 seconds; peak allocated memory: 444.751 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.355 ; gain = 93.730
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.355 ; gain = 93.730
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:09 . Memory (MB): peak = 185.355 ; gain = 93.730
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:10 . Memory (MB): peak = 185.355 ; gain = 93.730
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' for pipelining.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:111) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:113) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:115) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:15 . Memory (MB): peak = 185.355 ; gain = 93.730
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.7'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.7' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.7'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.6'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.6' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.6'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.5'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.5' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.5'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio.1' (f_b_1/forw_back_LTL.c:99:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:207:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:217:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:223:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:228:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:225:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:226:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:227:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:18 ; elapsed = 00:00:26 . Memory (MB): peak = 307.848 ; gain = 216.223
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixBackPropagatio.1' to 'MatrixBackPropagatio_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 26.275 seconds; current allocated memory: 253.974 MB.
INFO: [HLS 200-434] Only 1 loops out of a total 2 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.232 seconds; current allocated memory: 254.377 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.473 seconds; current allocated memory: 254.784 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 255.156 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.351 seconds; current allocated memory: 255.469 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.173 seconds; current allocated memory: 255.887 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.324 seconds; current allocated memory: 256.688 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.499 seconds; current allocated memory: 257.800 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.611 seconds; current allocated memory: 258.275 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.156 seconds; current allocated memory: 258.576 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.185 seconds; current allocated memory: 258.678 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.146 seconds; current allocated memory: 258.807 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.19 seconds; current allocated memory: 259.034 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.161 seconds; current allocated memory: 259.277 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.347 seconds; current allocated memory: 259.585 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.196 seconds; current allocated memory: 259.957 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.315 seconds; current allocated memory: 260.228 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.152 seconds; current allocated memory: 260.565 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.232 seconds; current allocated memory: 260.766 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.143 seconds; current allocated memory: 261.022 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.297 seconds; current allocated memory: 261.332 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.184 seconds; current allocated memory: 261.728 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 262.030 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.178 seconds; current allocated memory: 262.359 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.215 seconds; current allocated memory: 262.498 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.127 seconds; current allocated memory: 262.626 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.59 seconds; current allocated memory: 263.876 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (10.1173ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i1_65', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (9.13 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.406 seconds; current allocated memory: 266.089 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.472 seconds; current allocated memory: 267.387 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.41 seconds; current allocated memory: 269.297 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 1.206 seconds; current allocated memory: 270.726 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_5ns_6ns_5ns_10_1_1' to 'forw_back_mac_muldEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.65 seconds; current allocated memory: 271.832 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.549 seconds; current allocated memory: 272.874 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptrunceOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 1.022 seconds; current allocated memory: 275.262 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_3ns_6ns_3ns_7_1_1' to 'forw_back_mac_mullbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mullbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.362 seconds; current allocated memory: 276.401 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.462 seconds; current allocated memory: 276.780 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.301 seconds; current allocated memory: 277.291 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.505 seconds; current allocated memory: 278.204 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.624 seconds; current allocated memory: 279.085 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.482 seconds; current allocated memory: 279.695 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.532 seconds; current allocated memory: 280.624 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.696 seconds; current allocated memory: 281.588 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32ncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32ncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio_1'.
INFO: [HLS 200-111]  Elapsed time: 0.558 seconds; current allocated memory: 282.018 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gravdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_8ns_7ns_6ns_13_1_1' to 'forw_back_mac_mulyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_10ns_9ns_8ns_17_1_1' to 'forw_back_mac_mulzec' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuxdS': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulyd2': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulzec': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.772 seconds; current allocated memory: 285.682 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_keBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3Mgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiNgs' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.606 seconds; current allocated memory: 290.295 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_3_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gocq_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grarcU_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gratde_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gravdy_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dAem_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_keBew_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddEe0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddFfa_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddGfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouHfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouIfE_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Lf8_ram (RAM)' using block RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:47 ; elapsed = 00:01:01 . Memory (MB): peak = 397.844 ; gain = 306.219
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 61.083 seconds; peak allocated memory: 290.295 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:02 ; elapsed = 00:00:08 . Memory (MB): peak = 185.230 ; gain = 93.629
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:02 ; elapsed = 00:00:08 . Memory (MB): peak = 185.230 ; gain = 93.629
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:09 . Memory (MB): peak = 185.230 ; gain = 93.629
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:10 . Memory (MB): peak = 185.230 ; gain = 93.629
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' for pipelining.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:111) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:113) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:115) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:15 . Memory (MB): peak = 185.230 ; gain = 93.629
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.7'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.7' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.7'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.6'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.6' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.6'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.5'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.5' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.5'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio.1' (f_b_1/forw_back_LTL.c:99:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:5). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:207:2). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:217:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:222:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:227:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:228:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:224:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:225:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:226:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:18 ; elapsed = 00:00:26 . Memory (MB): peak = 307.516 ; gain = 215.914
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixBackPropagatio.1' to 'MatrixBackPropagatio_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 26.911 seconds; current allocated memory: 253.973 MB.
INFO: [HLS 200-434] Only 1 loops out of a total 2 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.171 seconds; current allocated memory: 254.376 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.332 seconds; current allocated memory: 254.783 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.225 seconds; current allocated memory: 255.155 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.314 seconds; current allocated memory: 255.468 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.171 seconds; current allocated memory: 255.886 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.335 seconds; current allocated memory: 256.703 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.501 seconds; current allocated memory: 257.815 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.625 seconds; current allocated memory: 258.274 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.163 seconds; current allocated memory: 258.575 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.194 seconds; current allocated memory: 258.693 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.115 seconds; current allocated memory: 258.822 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.194 seconds; current allocated memory: 259.049 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.235 seconds; current allocated memory: 259.292 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.303 seconds; current allocated memory: 259.584 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.172 seconds; current allocated memory: 259.956 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.387 seconds; current allocated memory: 260.243 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.155 seconds; current allocated memory: 260.580 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.236 seconds; current allocated memory: 260.750 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.143 seconds; current allocated memory: 261.005 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.313 seconds; current allocated memory: 261.347 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.174 seconds; current allocated memory: 261.743 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.333 seconds; current allocated memory: 262.029 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.165 seconds; current allocated memory: 262.358 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.212 seconds; current allocated memory: 262.513 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.138 seconds; current allocated memory: 262.640 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.572 seconds; current allocated memory: 263.875 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (10.1173ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i1_65', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (9.13 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.409 seconds; current allocated memory: 266.073 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.444 seconds; current allocated memory: 267.370 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.314 seconds; current allocated memory: 269.280 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 1.162 seconds; current allocated memory: 270.725 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_5ns_6ns_5ns_10_1_1' to 'forw_back_mac_muldEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.637 seconds; current allocated memory: 271.831 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.557 seconds; current allocated memory: 272.857 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptrunceOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.839 seconds; current allocated memory: 275.261 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_3ns_6ns_3ns_7_1_1' to 'forw_back_mac_mullbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mullbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.193 seconds; current allocated memory: 276.400 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.497 seconds; current allocated memory: 276.779 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.295 seconds; current allocated memory: 277.290 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.491 seconds; current allocated memory: 278.219 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.617 seconds; current allocated memory: 279.084 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.477 seconds; current allocated memory: 279.710 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.477 seconds; current allocated memory: 280.623 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.711 seconds; current allocated memory: 281.571 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32ncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32ncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio_1'.
INFO: [HLS 200-111]  Elapsed time: 0.63 seconds; current allocated memory: 282.017 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gravdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_8ns_7ns_6ns_13_1_1' to 'forw_back_mac_mulyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_10ns_9ns_8ns_17_1_1' to 'forw_back_mac_mulzec' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuxdS': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulyd2': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulzec': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.794 seconds; current allocated memory: 285.681 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_keBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3Mgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiNgs' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.657 seconds; current allocated memory: 290.294 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_3_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gocq_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grarcU_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gratde_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gravdy_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dAem_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_keBew_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddEe0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddFfa_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddGfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouHfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouIfE_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Lf8_ram (RAM)' using block RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:47 ; elapsed = 00:01:01 . Memory (MB): peak = 397.477 ; gain = 305.875
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 61.534 seconds; peak allocated memory: 290.294 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.445 ; gain = 93.914
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.445 ; gain = 93.914
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:10 . Memory (MB): peak = 185.445 ; gain = 93.914
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:11 . Memory (MB): peak = 185.445 ; gain = 93.914
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' for pipelining.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:111) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:113) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:115) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:15 . Memory (MB): peak = 185.445 ; gain = 93.914
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.7'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.7' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.7'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.6'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.6' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.6'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.5'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.5' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.5'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio.1' (f_b_1/forw_back_LTL.c:99:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:216:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:226:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:232:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:234:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:233:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:231:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:17 ; elapsed = 00:00:26 . Memory (MB): peak = 307.168 ; gain = 215.637
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixBackPropagatio.1' to 'MatrixBackPropagatio_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 26.756 seconds; current allocated memory: 253.403 MB.
INFO: [HLS 200-434] Only 1 loops out of a total 2 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.174 seconds; current allocated memory: 253.807 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.465 seconds; current allocated memory: 254.214 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.169 seconds; current allocated memory: 254.585 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.34 seconds; current allocated memory: 254.899 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.173 seconds; current allocated memory: 255.317 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.365 seconds; current allocated memory: 256.133 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.539 seconds; current allocated memory: 257.245 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.907 seconds; current allocated memory: 257.705 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.343 seconds; current allocated memory: 258.005 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.194 seconds; current allocated memory: 258.124 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.113 seconds; current allocated memory: 258.252 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 258.479 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.15 seconds; current allocated memory: 258.722 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.303 seconds; current allocated memory: 259.014 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.181 seconds; current allocated memory: 259.386 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.324 seconds; current allocated memory: 259.673 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.158 seconds; current allocated memory: 260.011 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 260.180 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.143 seconds; current allocated memory: 260.436 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.343 seconds; current allocated memory: 260.777 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.227 seconds; current allocated memory: 261.173 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.342 seconds; current allocated memory: 261.460 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.166 seconds; current allocated memory: 261.789 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.224 seconds; current allocated memory: 261.944 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.13 seconds; current allocated memory: 262.071 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.551 seconds; current allocated memory: 263.306 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (10.1173ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i1_64', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (9.13 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.566 seconds; current allocated memory: 265.503 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.392 seconds; current allocated memory: 266.797 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.387 seconds; current allocated memory: 268.697 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 1.145 seconds; current allocated memory: 270.142 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_5ns_6ns_5ns_10_1_1' to 'forw_back_mac_muldEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.713 seconds; current allocated memory: 271.247 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.591 seconds; current allocated memory: 272.289 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptrunceOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.841 seconds; current allocated memory: 274.677 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_3ns_6ns_3ns_7_1_1' to 'forw_back_mac_mullbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mullbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.229 seconds; current allocated memory: 275.816 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.533 seconds; current allocated memory: 276.195 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.289 seconds; current allocated memory: 276.706 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.505 seconds; current allocated memory: 277.635 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.616 seconds; current allocated memory: 278.500 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.492 seconds; current allocated memory: 279.126 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.477 seconds; current allocated memory: 280.039 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.779 seconds; current allocated memory: 280.988 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32ncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32ncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio_1'.
INFO: [HLS 200-111]  Elapsed time: 0.611 seconds; current allocated memory: 281.434 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gravdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_8ns_7ns_6ns_13_1_1' to 'forw_back_mac_mulyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_10ns_9ns_8ns_17_1_1' to 'forw_back_mac_mulzec' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuxdS': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulyd2': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulzec': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.82 seconds; current allocated memory: 285.082 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_keAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3Mgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiNgs' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.692 seconds; current allocated memory: 289.609 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_3_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gocq_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grarcU_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gratde_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gravdy_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_keAem_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddDeQ_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddEe0_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddFfa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dGfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouHfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouIfE_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Lf8_ram (RAM)' using block RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:46 ; elapsed = 00:01:02 . Memory (MB): peak = 398.164 ; gain = 306.633
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 62.872 seconds; peak allocated memory: 289.609 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [COSIM 212-47] Using XSIM for RTL simulation.
INFO: [COSIM 212-14] Instrumenting C test bench ...
WARNING: [COSIM 212-369] AXI_master port 'in' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv1' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv2' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'conv3' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc1' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc2' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'fc3' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'out' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
WARNING: [COSIM 212-369] AXI_master port 'lr' has a depth of '32'. Insufficient depth may result in simulation mismatch or freeze.
INFO: [COSIM 212-302] Starting C TB testing ... 
ERROR: [COSIM 212-330] Aborting co-simulation: top function 'forw_back' is not invoked in the test bench.
ERROR: [COSIM 212-5] *** C/RTL co-simulation file generation failed. ***
ERROR: [COSIM 212-4] *** C/RTL co-simulation finished: FAIL ***
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [IMPL 213-8] Exporting RTL as a Vivado IP.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_1/forw_back_LTL.c' ... 
WARNING: [HLS 200-40] In file included from f_b_1/forw_back_LTL.c:1:
f_b_1/forw_back_LTL.c:106:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:106:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:106:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:107:20: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:107:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:107:45: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(28,28,3,conv_out_1,conv_kernel_2,conv_out_2);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:108:20: warning: incompatible pointer types passing 'float [26][26]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                   ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:108:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                              ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:30:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_1/forw_back_LTL.c:108:45: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(26,26,3,conv_out_2,conv_kernel_3,conv_out_3);
                                            ^~~~~~~~~~
f_b_1/forw_back_LTL.c:30:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_1/forw_back_LTL.c:110:35: warning: incompatible pointer types passing 'float [24][24]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                  ^~~~~~~~~~
f_b_1/forw_back_LTL.c:40:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_1/forw_back_LTL.c:110:46: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(24,24,conv_out_3,fc_in_1);
                                             ^~~~~~~
f_b_1/forw_back_LTL.c:40:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_1/forw_back_LTL.c:111:28: warning: incompatible pointer types passing 'float [1][576]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                           ^~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:111:36: warning: incompatible pointer types passing 'float [576][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                   ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:111:53: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(576,180,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                    ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:112:14: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
             ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:112:23: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(180,fc_out_1,fc_in_2_relu1);
                      ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:27: warning: incompatible pointer types passing 'float [1][180]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                          ^~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:113:41: warning: incompatible pointer types passing 'float [180][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                        ^~~~~~~~~~~~~~~~
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:113:58: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(180,45,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                                                         ^~~~~~~~
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:114:13: warning: incompatible pointer types passing 'float [1][45]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(45,fc_out_2,fc_in_3_relu2);
            ^~~~~~~~
f_b_1/forw_back_LTL.c:54:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_1/forw_back_LTL.c:54:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_1/forw_back_LTL.c:46:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_1/forw_back_LTL.c:46:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:74:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_1/forw_back_LTL.c:60:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_1/forw_back_LTL.c:66:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:82:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_1/forw_back_LTL.c:30:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
f_b_1/forw_back_LTL.c:98:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:11 . Memory (MB): peak = 185.875 ; gain = 118.867
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:11 . Memory (MB): peak = 185.875 ; gain = 118.867
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:12 . Memory (MB): peak = 185.875 ; gain = 118.867
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:14 . Memory (MB): peak = 185.875 ; gain = 118.867
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label5' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' for pipelining.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:90) in function 'Padding.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.2' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.4' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.1': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1' (f_b_1/forw_back_LTL.c:62) in function 'MatrixBackPropagationMultiply.2': cannot completely unroll a loop with a variable trip count.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.5' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.6' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_1/forw_back_LTL.c:35) in function 'Conv2d.7' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_3' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_3_relu2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu.1' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_1/forw_back_LTL.c:56) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_1/forw_back_LTL.c:110) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.2' into 'forward' (f_b_1/forw_back_LTL.c:111) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu.1' into 'forward' (f_b_1/forw_back_LTL.c:112) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_1/forw_back_LTL.c:113) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_1/forw_back_LTL.c:114) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_1/forw_back_LTL.c:115) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.2' into 'backward' (f_b_1/forw_back_LTL.c:134) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.2' into 'backward' (f_b_1/forw_back_LTL.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_1/forw_back_LTL.c:141) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_1/forw_back_LTL.c:144) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:146) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_1/forw_back_LTL.c:148) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_1/forw_back_LTL.c:151) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.2' into 'backward' (f_b_1/forw_back_LTL.c:179) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_1/forw_back_LTL.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_1/forw_back_LTL.c:181) automatically.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding.1' (f_b_1/forw_back_LTL.c:89:34)...3 expression(s) balanced.
INFO: [XFORM 203-11] Balancing expressions in function 'Padding' (f_b_1/forw_back_LTL.c:88)...3 expression(s) balanced.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:18 . Memory (MB): peak = 185.875 ; gain = 118.867
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'MatrixBackPropagationMultiply_label2' (f_b_1/forw_back_LTL.c:62:10) in function 'backward'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Padding_label5' (f_b_1/forw_back_LTL.c:90:10) in function 'Padding'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.7'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.7' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.7'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.6'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.6' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.6'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.5'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.5' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.5'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_1/forw_back_LTL.c:35:18) in function 'Conv2d'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_1/forw_back_LTL.c:32:20) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_1/forw_back_LTL.c:31:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.3' to 'MatrixBackPropagatio.1' (f_b_1/forw_back_LTL.c:99:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:211:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:213:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:212:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:208:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:209:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:210:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_1/forw_back_LTL.c:216:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_1/forw_back_LTL.c:226:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 103680 on port 'data' (f_b_1/forw_back_LTL.c:232:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 450 on port 'data' (f_b_1/forw_back_LTL.c:234:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 8100 on port 'data' (f_b_1/forw_back_LTL.c:233:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:229:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:230:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_1/forw_back_LTL.c:231:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:17 ; elapsed = 00:00:29 . Memory (MB): peak = 307.141 ; gain = 240.133
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.7' to 'Conv2d_7'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.6' to 'Conv2d_6'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.5' to 'Conv2d_5'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'Padding.1' to 'Padding_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'MatrixBackPropagatio.1' to 'MatrixBackPropagatio_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:83): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:89): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:31): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:99): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_1/forw_back_LTL.c:186): 'return' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 29.33 seconds; current allocated memory: 253.403 MB.
INFO: [HLS 200-434] Only 1 loops out of a total 2 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.205 seconds; current allocated memory: 253.807 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.439 seconds; current allocated memory: 254.214 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.161 seconds; current allocated memory: 254.585 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.32 seconds; current allocated memory: 254.899 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.173 seconds; current allocated memory: 255.317 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.338 seconds; current allocated memory: 256.118 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.497 seconds; current allocated memory: 257.229 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.642 seconds; current allocated memory: 257.705 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.161 seconds; current allocated memory: 258.005 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.198 seconds; current allocated memory: 258.108 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.156 seconds; current allocated memory: 258.237 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.193 seconds; current allocated memory: 258.464 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.147 seconds; current allocated memory: 258.707 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.347 seconds; current allocated memory: 259.014 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.233 seconds; current allocated memory: 259.386 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.318 seconds; current allocated memory: 259.658 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.159 seconds; current allocated memory: 259.995 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label5_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.242 seconds; current allocated memory: 260.196 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.148 seconds; current allocated memory: 260.451 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.304 seconds; current allocated memory: 260.761 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.179 seconds; current allocated memory: 261.158 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36) and 'fadd' operation ('tmp_s', f_b_1/forw_back_LTL.c:36).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.363 seconds; current allocated memory: 261.460 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.172 seconds; current allocated memory: 261.789 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.228 seconds; current allocated memory: 261.928 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.176 seconds; current allocated memory: 262.055 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-61] Pipelining loop 'MatrixBackPropagationMultiply_label2_L'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.637 seconds; current allocated memory: 263.306 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] After resource sharing, estimated clock period (10.1173ns) exceeds the target (target clock period: 10ns, clock uncertainty: 1.25ns, effective delay budget: 8.75ns).
INFO: [BIND 205-100] The critical path consists of the following:
	'load' operation ('fc_hidden_layer2_loa_1', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) on array 'fc_hidden_layer2' (0.99 ns)
	'fsub' operation ('tmp_i1_64', f_b_1/forw_back_LTL.c:101->f_b_1/forw_back_LTL.c:180) (9.13 ns)
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.372 seconds; current allocated memory: 265.519 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv3.conv_kernel_3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc3.fc_hidden_layer3.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_3.conv3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer3.fc3'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.49 seconds; current allocated memory: 266.813 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.311 seconds; current allocated memory: 268.712 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_7' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_7'.
INFO: [HLS 200-111]  Elapsed time: 1.123 seconds; current allocated memory: 270.142 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_6' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_5ns_6ns_5ns_10_1_1' to 'forw_back_mac_muldEe' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_6'.
INFO: [HLS 200-111]  Elapsed time: 0.614 seconds; current allocated memory: 271.263 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_5' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_5'.
INFO: [HLS 200-111]  Elapsed time: 0.61 seconds; current allocated memory: 272.305 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptrunceOg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3fYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32g8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64kbM' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 274.693 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_3ns_6ns_3ns_7_1_1' to 'forw_back_mac_mullbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mullbW': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.191 seconds; current allocated memory: 275.817 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'OverturnKernel' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'OverturnKernel'.
INFO: [HLS 200-111]  Elapsed time: 0.461 seconds; current allocated memory: 276.195 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding_1'.
INFO: [HLS 200-111]  Elapsed time: 0.3 seconds; current allocated memory: 276.706 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_muldEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.553 seconds; current allocated memory: 277.619 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_mul_sub_4s_6ns_2ns_8_1_1' to 'forw_back_mac_mulmb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulmb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.701 seconds; current allocated memory: 278.501 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.488 seconds; current allocated memory: 279.126 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.486 seconds; current allocated memory: 280.039 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.667 seconds; current allocated memory: 280.988 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32ncg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32ncg': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio_1'.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 281.418 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_3_overtu' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_3_padding' to 'backward_conv_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gsc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_gratde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gudo' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_gravdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gwdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_8ns_7ns_6ns_13_1_1' to 'forw_back_mac_mulyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mac_muladd_10ns_9ns_8ns_17_1_1' to 'forw_back_mac_mulzec' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuxdS': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32g8j': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3fYi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptrunceOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulyd2': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_mac_mulzec': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 285.082 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc3' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_keAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_keBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_3' to 'forw_back_conv_keCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddDeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddEe0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer3' to 'forw_back_fc_hiddFfa' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dGfk' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_1' to 'forw_back_conv_ouHfu' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_out_2' to 'forw_back_conv_ouIfE' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_JfO' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2KfY' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_2_0' to 'forw_back_fc_out_Lf8' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_3_relu2_0' to 'forw_back_fc_in_3Mgi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiNgs' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'conv3', 'fc1', 'fc2', 'fc3', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.676 seconds; current allocated memory: 289.609 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_3_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_3_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_3_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_3_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gocq_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grarcU_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gratde_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_gravdy_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_keAem_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddDeQ_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddEe0_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddFfa_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dGfk_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouHfu_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_ouIfE_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_JfO_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_Lf8_ram (RAM)' using block RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:45 ; elapsed = 00:01:04 . Memory (MB): peak = 397.688 ; gain = 330.680
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 64.316 seconds; peak allocated memory: 289.609 MB.
