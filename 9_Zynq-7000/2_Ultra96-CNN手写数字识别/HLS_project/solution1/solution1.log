==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_4_new_network/forw_back_new_network.c' ... 
WARNING: [HLS 200-40] In file included from f_b_4_new_network/forw_back_new_network.c:1:
f_b_4_new_network/forw_back_new_network.c:132:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_4_new_network/forw_back_new_network.c:132:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_4_new_network/forw_back_new_network.c:132:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_4_new_network/forw_back_new_network.c:133:23: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(28,28,2,conv_out_1,max_poo_out_1,max_poo_locate_1);
                      ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:41: note: passing argument to parameter 'input_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                        ^
f_b_4_new_network/forw_back_new_network.c:133:34: warning: incompatible pointer types passing 'float [14][14]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(28,28,2,conv_out_1,max_poo_out_1,max_poo_locate_1);
                                 ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:61: note: passing argument to parameter 'output_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                            ^
f_b_4_new_network/forw_back_new_network.c:133:48: warning: incompatible pointer types passing 'float [14][14]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(28,28,2,conv_out_1,max_poo_out_1,max_poo_locate_1);
                                               ^~~~~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:82: note: passing argument to parameter 'locate_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                                                 ^
f_b_4_new_network/forw_back_new_network.c:134:20: warning: incompatible pointer types passing 'float [14][14]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(14,14,3,max_poo_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_4_new_network/forw_back_new_network.c:134:34: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(14,14,3,max_poo_out_1,conv_kernel_2,conv_out_2);
                                 ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_4_new_network/forw_back_new_network.c:134:48: warning: incompatible pointer types passing 'float [12][12]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(14,14,3,max_poo_out_1,conv_kernel_2,conv_out_2);
                                               ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_4_new_network/forw_back_new_network.c:135:23: warning: incompatible pointer types passing 'float [12][12]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(12,12,2,conv_out_2,max_poo_out_2,max_poo_locate_2);
                      ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:41: note: passing argument to parameter 'input_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                        ^
f_b_4_new_network/forw_back_new_network.c:135:34: warning: incompatible pointer types passing 'float [6][6]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(12,12,2,conv_out_2,max_poo_out_2,max_poo_locate_2);
                                 ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:61: note: passing argument to parameter 'output_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                            ^
f_b_4_new_network/forw_back_new_network.c:135:48: warning: incompatible pointer types passing 'float [6][6]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(12,12,2,conv_out_2,max_poo_out_2,max_poo_locate_2);
                                               ^~~~~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:82: note: passing argument to parameter 'locate_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                                                 ^
f_b_4_new_network/forw_back_new_network.c:137:33: warning: incompatible pointer types passing 'float [6][6]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(6,6,max_poo_out_2,fc_in_1);
                                ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:51:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_4_new_network/forw_back_new_network.c:137:47: warning: incompatible pointer types passing 'float [1][36]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(6,6,max_poo_out_2,fc_in_1);
                                              ^~~~~~~
f_b_4_new_network/forw_back_new_network.c:51:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_4_new_network/forw_back_new_network.c:138:26: warning: incompatible pointer types passing 'float [1][36]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(36,20,fc_in_1,fc_hidden_layer1,fc_out_1);
                         ^~~~~~~
f_b_4_new_network/forw_back_new_network.c:57:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_4_new_network/forw_back_new_network.c:138:34: warning: incompatible pointer types passing 'float [36][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(36,20,fc_in_1,fc_hidden_layer1,fc_out_1);
                                 ^~~~~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:57:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_4_new_network/forw_back_new_network.c:138:51: warning: incompatible pointer types passing 'float [1][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(36,20,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                  ^~~~~~~~
f_b_4_new_network/forw_back_new_network.c:57:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_4_new_network/forw_back_new_network.c:139:13: warning: incompatible pointer types passing 'float [1][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(20,fc_out_1,fc_in_2_relu1);
            ^~~~~~~~
f_b_4_new_network/forw_back_new_network.c:65:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_4_new_network/forw_back_new_network.c:139:22: warning: incompatible pointer types passing 'float [1][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(20,fc_out_1,fc_in_2_relu1);
                     ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:65:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_4_new_network/forw_back_new_network.c:140:26: warning: incompatible pointer types passing 'float [1][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(20,10,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                         ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:57:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_4_new_network/forw_back_new_network.c:57:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_4_new_network/forw_back_new_network.c:57:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_4_new_network/forw_back_new_network.c:72:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_4_new_network/forw_back_new_network.c:78:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_4_new_network/forw_back_new_network.c:86:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_4_new_network/forw_back_new_network.c:72:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_4_new_network/forw_back_new_network.c:78:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_4_new_network/forw_back_new_network.c:94:94: note: passing argument to parameter 'locate_matrix' here
void MaxPooBackPropagation(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                                                             ^
f_b_4_new_network/forw_back_new_network.c:27:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_4_new_network/forw_back_new_network.c:106:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_4_new_network/forw_back_new_network.c:94:94: note: passing argument to parameter 'locate_matrix' here
void MaxPooBackPropagation(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                                                             ^
f_b_4_new_network/forw_back_new_network.c:27:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_4_new_network/forw_back_new_network.c:122:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_4_new_network/forw_back_new_network.c:122:67: note: passing argument to parameter 'output_matrix' here
f_b_4_new_network/forw_back_new_network.c:122:67: note: passing argument to parameter 'output_matrix' here
f_b_4_new_network/forw_back_new_network.c:122:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:10 . Memory (MB): peak = 185.441 ; gain = 89.684
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:10 . Memory (MB): peak = 185.441 ; gain = 89.684
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:12 . Memory (MB): peak = 188.973 ; gain = 93.215
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::mantissa' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:15) automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::expv' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:18) automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::__signbit' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:59) automatically.
INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' into 'generic_cast_IEEE754<int, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:117) automatically.
INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, float>' into '__hls_fptosi_float_i32' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/src/lib_floatconversion.cpp:51) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPool2d.1' (f_b_4_new_network/forw_back_new_network.c:44) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPool2d' (f_b_4_new_network/forw_back_new_network.c:44) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_4_new_network/forw_back_new_network.c:67) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_4_new_network/forw_back_new_network.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_4_new_network/forw_back_new_network.c:139) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPooBackPropagation.1' (f_b_4_new_network/forw_back_new_network.c:101) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPooBackPropagation' (f_b_4_new_network/forw_back_new_network.c:101) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:159) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:162) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_4_new_network/forw_back_new_network.c:164) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_4_new_network/forw_back_new_network.c:166) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_4_new_network/forw_back_new_network.c:168) automatically.
INFO: [XFORM 203-602] Inlining function 'OverturnKernel' into 'backward' (f_b_4_new_network/forw_back_new_network.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:192) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_4_new_network/forw_back_new_network.c:193) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:13 . Memory (MB): peak = 207.227 ; gain = 111.469
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label3' (f_b_4_new_network/forw_back_new_network.c:114) in function 'Padding' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixExtensionImproved_label1' (f_b_4_new_network/forw_back_new_network.c:53) in function 'MatrixExtensionImproved' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.4' for pipelining.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:114) in function 'Padding' completely with a factor of 16.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.2' completely: variable loop bound.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:53) in function 'MatrixExtensionImproved' completely with a factor of 6.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.4' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::mantissa' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:15) automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::expv' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:18) automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::__signbit' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:59) automatically.
INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' into 'generic_cast_IEEE754<int, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:117) automatically.
INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, float>' into '__hls_fptosi_float_i32' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/src/lib_floatconversion.cpp:51) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPool2d.1' (f_b_4_new_network/forw_back_new_network.c:44) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPool2d' (f_b_4_new_network/forw_back_new_network.c:44) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_4_new_network/forw_back_new_network.c:67) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_4_new_network/forw_back_new_network.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_4_new_network/forw_back_new_network.c:138) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_4_new_network/forw_back_new_network.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_4_new_network/forw_back_new_network.c:140) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPooBackPropagation.1' (f_b_4_new_network/forw_back_new_network.c:101) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPooBackPropagation' (f_b_4_new_network/forw_back_new_network.c:101) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:159) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:162) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_4_new_network/forw_back_new_network.c:164) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_4_new_network/forw_back_new_network.c:166) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_4_new_network/forw_back_new_network.c:168) automatically.
INFO: [XFORM 203-602] Inlining function 'OverturnKernel' into 'backward' (f_b_4_new_network/forw_back_new_network.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:192) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_4_new_network/forw_back_new_network.c:193) automatically.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:18 . Memory (MB): peak = 257.320 ; gain = 161.562
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MaxPooBackPropagation.1' to 'MaxPooBackPropagatio' (f_b_4_new_network/forw_back_new_network.c:15:27)
WARNING: [XFORM 203-631] Renaming function 'MaxPooBackPropagation' to 'MaxPooBackPropagatio.1' (f_b_4_new_network/forw_back_new_network.c:15:27)
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.2' to 'MatrixBackPropagatio' (f_b_4_new_network/forw_back_new_network.c:123:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 200 on port 'data' (f_b_4_new_network/forw_back_new_network.c:219:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 720 on port 'data' (f_b_4_new_network/forw_back_new_network.c:218:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_4_new_network/forw_back_new_network.c:216:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_4_new_network/forw_back_new_network.c:217:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_4_new_network/forw_back_new_network.c:222:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_4_new_network/forw_back_new_network.c:232:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 200 on port 'data' (f_b_4_new_network/forw_back_new_network.c:238:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 720 on port 'data' (f_b_4_new_network/forw_back_new_network.c:237:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_4_new_network/forw_back_new_network.c:235:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_4_new_network/forw_back_new_network.c:236:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:13 ; elapsed = 00:00:24 . Memory (MB): peak = 386.492 ; gain = 290.734
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'MaxPool2d.1' to 'MaxPool2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'MaxPooBackPropagatio.1' to 'MaxPooBackPropagatio_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:38): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:38): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:95): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:113): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:95): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:123): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 24.27 seconds; current allocated memory: 328.898 MB.
INFO: [HLS 200-434] Only 1 loops out of a total 2 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.198 seconds; current allocated memory: 329.317 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MaxPool2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.298 seconds; current allocated memory: 329.715 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.201 seconds; current allocated memory: 330.084 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.399 seconds; current allocated memory: 330.398 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.191 seconds; current allocated memory: 330.794 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MaxPool2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.264 seconds; current allocated memory: 331.161 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.178 seconds; current allocated memory: 331.530 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixExtensionImproved_label1'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('max_poo_out_2_load_2', f_b_4_new_network/forw_back_new_network.c:54->f_b_4_new_network/forw_back_new_network.c:137) on array 'max_poo_out_2' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'max_poo_out_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 3, Depth = 4.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.4 seconds; current allocated memory: 332.245 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.777 seconds; current allocated memory: 333.416 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MaxPooBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.648 seconds; current allocated memory: 333.917 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.208 seconds; current allocated memory: 334.160 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.284 seconds; current allocated memory: 334.399 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.29 seconds; current allocated memory: 334.764 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_matrix_load_2', f_b_4_new_network/forw_back_new_network.c:116) on array 'input_matrix' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_matrix'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('output_matrix_addr_23_write_ln118', f_b_4_new_network/forw_back_new_network.c:118) of constant 0 on array 'output_matrix' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_matrix'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 8, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.451 seconds; current allocated memory: 335.377 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.278 seconds; current allocated memory: 335.921 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.405 seconds; current allocated memory: 336.203 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.188 seconds; current allocated memory: 336.622 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MaxPooBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.264 seconds; current allocated memory: 336.904 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.165 seconds; current allocated memory: 337.146 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.313 seconds; current allocated memory: 337.420 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.189 seconds; current allocated memory: 337.749 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.243 seconds; current allocated memory: 337.868 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.206 seconds; current allocated memory: 338.031 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.38 seconds; current allocated memory: 338.883 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.976 seconds; current allocated memory: 340.353 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.035 seconds; current allocated memory: 341.250 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.098 seconds; current allocated memory: 342.671 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 1.034 seconds; current allocated memory: 343.964 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MaxPool2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_sitofp_32ns_32_4_1' to 'forw_back_sitofp_dEe' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32eOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_sitofp_dEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MaxPool2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.734 seconds; current allocated memory: 344.956 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.5 seconds; current allocated memory: 345.890 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MaxPool2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_sitofp_dEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MaxPool2d'.
INFO: [HLS 200-111]  Elapsed time: 0.602 seconds; current allocated memory: 346.815 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forward_max_poo_out_2' to 'forward_max_poo_ofYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptruncg8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64kbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64lbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64lbW': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncg8j': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.771 seconds; current allocated memory: 349.033 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MaxPooBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'MaxPooBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 1.245 seconds; current allocated memory: 349.908 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.445 seconds; current allocated memory: 350.679 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.57 seconds; current allocated memory: 351.657 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.831 seconds; current allocated memory: 352.638 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MaxPooBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'MaxPooBackPropagatio_1'.
INFO: [HLS 200-111]  Elapsed time: 0.619 seconds; current allocated memory: 353.355 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.58 seconds; current allocated memory: 354.128 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32mb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32mb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 354.574 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grancg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_pool_grad_1' to 'backward_pool_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_grasc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gtde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuudo' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuudo': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncg8j': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.642 seconds; current allocated memory: 357.275 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_kevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_kewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_max_poo_out_1' to 'forw_back_max_pooAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_max_poo_locate_1' to 'forw_back_max_pooBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_max_poo_locate_2' to 'forw_back_max_pooCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2Ee0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiFfa' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'fc1', 'fc2', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.148 seconds; current allocated memory: 360.826 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_1_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_2_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_max_poo_ofYi_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_2_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_1_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grancg_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gocq_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_pool_grarcU_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grasc4_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_kevdy_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddxdS_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddyd2_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dzec_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_max_pooAem_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_max_pooCeG_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_in_1_0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_DeQ_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:39 ; elapsed = 00:00:57 . Memory (MB): peak = 469.414 ; gain = 373.656
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 57.181 seconds; peak allocated memory: 360.826 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [IMPL 213-8] Exporting RTL as a Vivado IP.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'f_b_4_new_network/forw_back_new_network.c' ... 
WARNING: [HLS 200-40] In file included from f_b_4_new_network/forw_back_new_network.c:1:
f_b_4_new_network/forw_back_new_network.c:132:20: warning: incompatible pointer types passing 'float [30][30]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                   ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_4_new_network/forw_back_new_network.c:132:31: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                              ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_4_new_network/forw_back_new_network.c:132:45: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(30,30,3,mnist_data,conv_kernel_1,conv_out_1);
                                            ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_4_new_network/forw_back_new_network.c:133:23: warning: incompatible pointer types passing 'float [28][28]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(28,28,2,conv_out_1,max_poo_out_1,max_poo_locate_1);
                      ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:41: note: passing argument to parameter 'input_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                        ^
f_b_4_new_network/forw_back_new_network.c:133:34: warning: incompatible pointer types passing 'float [14][14]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(28,28,2,conv_out_1,max_poo_out_1,max_poo_locate_1);
                                 ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:61: note: passing argument to parameter 'output_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                            ^
f_b_4_new_network/forw_back_new_network.c:133:48: warning: incompatible pointer types passing 'float [14][14]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(28,28,2,conv_out_1,max_poo_out_1,max_poo_locate_1);
                                               ^~~~~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:82: note: passing argument to parameter 'locate_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                                                 ^
f_b_4_new_network/forw_back_new_network.c:134:20: warning: incompatible pointer types passing 'float [14][14]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(14,14,3,max_poo_out_1,conv_kernel_2,conv_out_2);
                   ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_4_new_network/forw_back_new_network.c:134:34: warning: incompatible pointer types passing 'float [3][3]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(14,14,3,max_poo_out_1,conv_kernel_2,conv_out_2);
                                 ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:58: note: passing argument to parameter 'kernel' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                         ^
f_b_4_new_network/forw_back_new_network.c:134:48: warning: incompatible pointer types passing 'float [12][12]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Conv2d(14,14,3,max_poo_out_1,conv_kernel_2,conv_out_2);
                                               ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:27:72: note: passing argument to parameter 'out_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                                                       ^
f_b_4_new_network/forw_back_new_network.c:135:23: warning: incompatible pointer types passing 'float [12][12]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(12,12,2,conv_out_2,max_poo_out_2,max_poo_locate_2);
                      ^~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:41: note: passing argument to parameter 'input_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                        ^
f_b_4_new_network/forw_back_new_network.c:135:34: warning: incompatible pointer types passing 'float [6][6]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(12,12,2,conv_out_2,max_poo_out_2,max_poo_locate_2);
                                 ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:61: note: passing argument to parameter 'output_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                            ^
f_b_4_new_network/forw_back_new_network.c:135:48: warning: incompatible pointer types passing 'float [6][6]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MaxPool2d(12,12,2,conv_out_2,max_poo_out_2,max_poo_locate_2);
                                               ^~~~~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:37:82: note: passing argument to parameter 'locate_matrix' here
void MaxPool2d(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                                                 ^
f_b_4_new_network/forw_back_new_network.c:137:33: warning: incompatible pointer types passing 'float [6][6]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(6,6,max_poo_out_2,fc_in_1);
                                ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:51:49: note: passing argument to parameter 'input_matrix1' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                ^
f_b_4_new_network/forw_back_new_network.c:137:47: warning: incompatible pointer types passing 'float [1][36]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixExtensionImproved(6,6,max_poo_out_2,fc_in_1);
                                              ^~~~~~~
f_b_4_new_network/forw_back_new_network.c:51:70: note: passing argument to parameter 'output_matrix' here
void MatrixExtensionImproved(int w,int h,float *input_matrix1,float *output_matrix){
                                                                     ^
f_b_4_new_network/forw_back_new_network.c:138:26: warning: incompatible pointer types passing 'float [1][36]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(36,20,fc_in_1,fc_hidden_layer1,fc_out_1);
                         ^~~~~~~
f_b_4_new_network/forw_back_new_network.c:57:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_4_new_network/forw_back_new_network.c:138:34: warning: incompatible pointer types passing 'float [36][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(36,20,fc_in_1,fc_hidden_layer1,fc_out_1);
                                 ^~~~~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:57:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_4_new_network/forw_back_new_network.c:138:51: warning: incompatible pointer types passing 'float [1][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(36,20,fc_in_1,fc_hidden_layer1,fc_out_1);
                                                  ^~~~~~~~
f_b_4_new_network/forw_back_new_network.c:57:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_4_new_network/forw_back_new_network.c:139:13: warning: incompatible pointer types passing 'float [1][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(20,fc_out_1,fc_in_2_relu1);
            ^~~~~~~~
f_b_4_new_network/forw_back_new_network.c:65:24: note: passing argument to parameter 'input_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                       ^
f_b_4_new_network/forw_back_new_network.c:139:22: warning: incompatible pointer types passing 'float [1][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    Relu(20,fc_out_1,fc_in_2_relu1);
                     ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:65:44: note: passing argument to parameter 'output_matrix' here
void Relu(int h,float *input_matrix,float *output_matrix){
                                           ^
f_b_4_new_network/forw_back_new_network.c:140:26: warning: incompatible pointer types passing 'float [1][20]' to parameter of type 'float *' [-Wincompatible-pointer-types]
    MatrixMultiply(20,10,fc_in_2_relu1,fc_hidden_layer2,fc_out_2);
                         ^~~~~~~~~~~~~
f_b_4_new_network/forw_back_new_network.c:57:44: note: passing argument to parameter 'input_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                           ^
f_b_4_new_network/forw_back_new_network.c:57:64: note: passing argument to parameter 'para_layer' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                               ^
f_b_4_new_network/forw_back_new_network.c:57:81: note: passing argument to parameter 'output_matrix' here
void MatrixMultiply(int h,int h_out,float *input_matrix,float *para_layer,float*output_matrix){
                                                                                ^
f_b_4_new_network/forw_back_new_network.c:72:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_4_new_network/forw_back_new_network.c:78:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_4_new_network/forw_back_new_network.c:86:39: note: passing argument to parameter 'input_matrix' here
void ReluBackPropagation(int w,float *input_matrix,float *grad,float *output_matrix){
                                      ^
f_b_4_new_network/forw_back_new_network.c:72:55: note: passing argument to parameter 'input_matrix' here
void MatrixBackPropagationMultiply(int w,int h,float *input_matrix,float *grad,float *rgrad){
                                                      ^
f_b_4_new_network/forw_back_new_network.c:78:45: note: passing argument to parameter 'input_matrix' here
void CalculateMatrixGrad(int w,int h,float *input_matrix,float *grad,float *output_matrix){
                                            ^
f_b_4_new_network/forw_back_new_network.c:94:94: note: passing argument to parameter 'locate_matrix' here
void MaxPooBackPropagation(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                                                             ^
f_b_4_new_network/forw_back_new_network.c:27:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_4_new_network/forw_back_new_network.c:106:34: note: passing argument to parameter 'input_matrix' here
void OverturnKernel(int k,float *input_matrix,float *output_matrix){
                                 ^
f_b_4_new_network/forw_back_new_network.c:94:94: note: passing argument to parameter 'locate_matrix' here
void MaxPooBackPropagation(int w,int h,int k,float *input_matrix,float *output_matrix,float *locate_matrix){
                                                                                             ^
f_b_4_new_network/forw_back_new_network.c:27:38: note: passing argument to parameter 'input_matrix' here
void Conv2d(int w,int h,int k,float *input_matrix,float *kernel,float *out_matrix){
                                     ^
f_b_4_new_network/forw_back_new_network.c:122:67: note: passing argument to parameter 'output_matrix' here
void MatrixBackPropagation(int w,int h,float *input_matrix,float *output_matrix,float lr){
                                                                  ^
f_b_4_new_network/forw_back_new_network.c:122:67: note: passing argument to parameter 'output_matrix' here
f_b_4_new_network/forw_back_new_network.c:122:67: note: passing argument to parameter 'output_matrix' here
f_b_4_new_network/forw_back_new_network.c:122:67: note: passing argument to parameter 'output_matrix' here
20 warnings generated.
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.613 ; gain = 94.047
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:01 ; elapsed = 00:00:08 . Memory (MB): peak = 185.613 ; gain = 94.047
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:02 ; elapsed = 00:00:11 . Memory (MB): peak = 189.738 ; gain = 98.172
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::mantissa' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:15) automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::expv' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:18) automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::__signbit' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:59) automatically.
INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' into 'generic_cast_IEEE754<int, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:117) automatically.
INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, float>' into '__hls_fptosi_float_i32' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/src/lib_floatconversion.cpp:51) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPool2d.1' (f_b_4_new_network/forw_back_new_network.c:44) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPool2d' (f_b_4_new_network/forw_back_new_network.c:44) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_4_new_network/forw_back_new_network.c:67) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_4_new_network/forw_back_new_network.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_4_new_network/forw_back_new_network.c:139) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPooBackPropagation.1' (f_b_4_new_network/forw_back_new_network.c:101) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPooBackPropagation' (f_b_4_new_network/forw_back_new_network.c:101) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:159) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:162) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_4_new_network/forw_back_new_network.c:164) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_4_new_network/forw_back_new_network.c:166) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_4_new_network/forw_back_new_network.c:168) automatically.
INFO: [XFORM 203-602] Inlining function 'OverturnKernel' into 'backward' (f_b_4_new_network/forw_back_new_network.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:192) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_4_new_network/forw_back_new_network.c:193) automatically.
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:03 ; elapsed = 00:00:12 . Memory (MB): peak = 207.578 ; gain = 116.012
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Padding_label3' (f_b_4_new_network/forw_back_new_network.c:114) in function 'Padding' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.1' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.2' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'MatrixExtensionImproved_label1' (f_b_4_new_network/forw_back_new_network.c:53) in function 'MatrixExtensionImproved' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.3' for pipelining.
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.4' for pipelining.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:114) in function 'Padding' completely with a factor of 16.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.1' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.2' completely: variable loop bound.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:53) in function 'MatrixExtensionImproved' completely with a factor of 6.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.3' completely: variable loop bound.
WARNING: [XFORM 203-503] Cannot unroll loop 'Loop-1.1.1.1' (f_b_4_new_network/forw_back_new_network.c:32) in function 'Conv2d.4' completely: variable loop bound.
INFO: [XFORM 203-102] Partitioning array 'fc_out_2' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_out_1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_2_relu1' in dimension 1 automatically.
INFO: [XFORM 203-102] Partitioning array 'fc_in_1' in dimension 1 automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::mantissa' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:15) automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::expv' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:18) automatically.
INFO: [XFORM 203-602] Inlining function 'fp_struct<float>::__signbit' into 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:59) automatically.
INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, (ap_q_mode)6, float>' into 'generic_cast_IEEE754<int, float>' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/include/FloatingPoint\hls_case_IEEE754.h:117) automatically.
INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, float>' into '__hls_fptosi_float_i32' (r:/builds/2019.1/continuous/2019_05_24_2552052/src/products/hls/hls_lib/hlsmath/src/lib_floatconversion.cpp:51) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPool2d.1' (f_b_4_new_network/forw_back_new_network.c:44) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPool2d' (f_b_4_new_network/forw_back_new_network.c:44) automatically.
INFO: [XFORM 203-602] Inlining function 'max' into 'Relu' (f_b_4_new_network/forw_back_new_network.c:67) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixExtensionImproved' into 'forward' (f_b_4_new_network/forw_back_new_network.c:137) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply.1' into 'forward' (f_b_4_new_network/forw_back_new_network.c:138) automatically.
INFO: [XFORM 203-602] Inlining function 'Relu' into 'forward' (f_b_4_new_network/forw_back_new_network.c:139) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixMultiply' into 'forward' (f_b_4_new_network/forw_back_new_network.c:140) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPooBackPropagation.1' (f_b_4_new_network/forw_back_new_network.c:101) automatically.
INFO: [XFORM 203-602] Inlining function '__hls_fptosi_float_i32' into 'MaxPooBackPropagation' (f_b_4_new_network/forw_back_new_network.c:101) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:159) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:162) automatically.
INFO: [XFORM 203-602] Inlining function 'ReluBackPropagation' into 'backward' (f_b_4_new_network/forw_back_new_network.c:164) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagationMultiply' into 'backward' (f_b_4_new_network/forw_back_new_network.c:166) automatically.
INFO: [XFORM 203-602] Inlining function 'CalculateMatrixGrad' into 'backward' (f_b_4_new_network/forw_back_new_network.c:168) automatically.
INFO: [XFORM 203-602] Inlining function 'OverturnKernel' into 'backward' (f_b_4_new_network/forw_back_new_network.c:180) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation.1' into 'backward' (f_b_4_new_network/forw_back_new_network.c:192) automatically.
INFO: [XFORM 203-602] Inlining function 'MatrixBackPropagation' into 'backward' (f_b_4_new_network/forw_back_new_network.c:193) automatically.
INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:16 . Memory (MB): peak = 258.043 ; gain = 166.477
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d.4'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d.4' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d.4'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d.3'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d.3' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d.3'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d.2'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d.2' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d.2'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d.1'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d.1' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d.1'.
INFO: [XFORM 203-541] Flattening a loop nest 'Conv2d_label0' (f_b_4_new_network/forw_back_new_network.c:32:18) in function 'Conv2d'.
WARNING: [XFORM 203-542] Cannot flatten a loop nest 'Loop-1.1' (f_b_4_new_network/forw_back_new_network.c:29:20) in function 'Conv2d' : 

the outer loop is not a perfect loop.
INFO: [XFORM 203-541] Flattening a loop nest 'Loop-1' (f_b_4_new_network/forw_back_new_network.c:28:16) in function 'Conv2d'.
WARNING: [XFORM 203-631] Renaming function 'MaxPooBackPropagation.1' to 'MaxPooBackPropagatio' (f_b_4_new_network/forw_back_new_network.c:15:27)
WARNING: [XFORM 203-631] Renaming function 'MaxPooBackPropagation' to 'MaxPooBackPropagatio.1' (f_b_4_new_network/forw_back_new_network.c:15:27)
WARNING: [XFORM 203-631] Renaming function 'MatrixBackPropagation.2' to 'MatrixBackPropagatio' (f_b_4_new_network/forw_back_new_network.c:123:25)
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 200 on port 'data' (f_b_4_new_network/forw_back_new_network.c:219:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 720 on port 'data' (f_b_4_new_network/forw_back_new_network.c:218:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_4_new_network/forw_back_new_network.c:216:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 9 on port 'data' (f_b_4_new_network/forw_back_new_network.c:217:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst read of a total cumulative length 900 on port 'data' (f_b_4_new_network/forw_back_new_network.c:222:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 10 on port 'data' (f_b_4_new_network/forw_back_new_network.c:232:3). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 200 on port 'data' (f_b_4_new_network/forw_back_new_network.c:238:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 720 on port 'data' (f_b_4_new_network/forw_back_new_network.c:237:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_4_new_network/forw_back_new_network.c:235:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-444] Inferring multiple bus burst write of a total cumulative length 9 on port 'data' (f_b_4_new_network/forw_back_new_network.c:236:9). These data requests might be further partitioned to multiple requests during RTL generation, based on max_read_burst_length or max_write_burst_length settings.
INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:13 ; elapsed = 00:00:22 . Memory (MB): peak = 387.426 ; gain = 295.859
INFO: [HLS 200-10] Starting hardware synthesis ...
INFO: [HLS 200-10] Synthesizing 'forw_back' ...
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.4' to 'Conv2d_4'.
WARNING: [SYN 201-103] Legalizing function name 'MaxPool2d.1' to 'MaxPool2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.3' to 'Conv2d_3'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.2' to 'Conv2d_2'.
WARNING: [SYN 201-103] Legalizing function name 'Conv2d.1' to 'Conv2d_1'.
WARNING: [SYN 201-103] Legalizing function name 'MaxPooBackPropagatio.1' to 'MaxPooBackPropagatio_1'.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:38): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:38): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:95): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:113): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:95): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:28): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-303] Cannot apply memory assignment of 'RAM_2P_LUTRAM' (f_b_4_new_network/forw_back_new_network.c:123): 'fc_hidden_layer2' does not exist or is optimized away.
WARNING: [SYN 201-107] Renaming port name 'forw_back/in' to 'forw_back/in_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/out' to 'forw_back/out_r' to avoid the conflict with HDL keywords or other object names.
WARNING: [SYN 201-107] Renaming port name 'forw_back/label' to 'forw_back/label_r' to avoid the conflict with HDL keywords or other object names.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 23.285 seconds; current allocated memory: 328.919 MB.
INFO: [HLS 200-434] Only 1 loops out of a total 2 loops have been pipelined in this design.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.207 seconds; current allocated memory: 329.322 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MaxPool2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.277 seconds; current allocated memory: 329.720 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.186 seconds; current allocated memory: 330.089 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.445 seconds; current allocated memory: 330.418 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.199 seconds; current allocated memory: 330.815 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MaxPool2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.297 seconds; current allocated memory: 331.166 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.18 seconds; current allocated memory: 331.535 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'MatrixExtensionImproved_label1'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('max_poo_out_2_load_2', f_b_4_new_network/forw_back_new_network.c:54->f_b_4_new_network/forw_back_new_network.c:137) on array 'max_poo_out_2' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'max_poo_out_2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 3, Depth = 4.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.452 seconds; current allocated memory: 332.245 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.793 seconds; current allocated memory: 333.430 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MaxPooBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 333.916 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.169 seconds; current allocated memory: 334.159 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.335 seconds; current allocated memory: 334.398 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.258 seconds; current allocated memory: 334.763 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Padding_label3'.
WARNING: [SCHED 204-69] Unable to schedule 'load' operation ('input_matrix_load_2', f_b_4_new_network/forw_back_new_network.c:116) on array 'input_matrix' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'input_matrix'.
WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('output_matrix_addr_23_write_ln118', f_b_4_new_network/forw_back_new_network.c:118) of constant 0 on array 'output_matrix' due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'output_matrix'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 8, Depth = 8.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.433 seconds; current allocated memory: 335.376 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.289 seconds; current allocated memory: 335.920 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 10.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.423 seconds; current allocated memory: 336.202 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.207 seconds; current allocated memory: 336.621 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MaxPooBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.279 seconds; current allocated memory: 336.902 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.168 seconds; current allocated memory: 337.145 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'Conv2d_label0_L'.
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 1)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 2)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
WARNING: [SCHED 204-68] Unable to enforce a carried constraint (II = 3)
   between 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33) and 'fadd' operation ('tmp_s', f_b_4_new_network/forw_back_new_network.c:33).
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 4, Depth = 9.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.325 seconds; current allocated memory: 337.419 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.182 seconds; current allocated memory: 337.748 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.234 seconds; current allocated memory: 337.867 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.249 seconds; current allocated memory: 337.993 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 0.363 seconds; current allocated memory: 338.881 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 0.963 seconds; current allocated memory: 340.352 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-42] -- Implementing module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SCHED 204-11] Starting scheduling ...
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv1.conv_kernel_1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv2.conv_kernel_2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc1.fc_hidden_layer1.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc2.fc_hidden_layer2.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.out.probability_result.gep'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.mnist_data.in'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_1.conv1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.conv_kernel_2.conv2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer1.fc1'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-61] Pipelining loop 'memcpy.fc_hidden_layer2.fc2'.
INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.
INFO: [SCHED 204-11] Finished scheduling.
INFO: [HLS 200-111]  Elapsed time: 1.057 seconds; current allocated memory: 341.265 MB.
INFO: [BIND 205-100] Starting micro-architecture generation ...
INFO: [BIND 205-101] Performing variable lifetime analysis.
INFO: [BIND 205-101] Exploring resource sharing.
INFO: [BIND 205-101] Binding ...
INFO: [BIND 205-100] Finished micro-architecture generation.
INFO: [HLS 200-111]  Elapsed time: 1.199 seconds; current allocated memory: 342.685 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_4' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fadd_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fadd_32bkb' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fmul_32ns_32ns_32_3_max_dsp_1' to 'forw_back_fmul_32cud' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_4'.
INFO: [HLS 200-111]  Elapsed time: 0.972 seconds; current allocated memory: 343.994 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MaxPool2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_sitofp_32ns_32_4_1' to 'forw_back_sitofp_dEe' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fcmp_32ns_32ns_1_2_1' to 'forw_back_fcmp_32eOg' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_sitofp_dEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MaxPool2d_1'.
INFO: [HLS 200-111]  Elapsed time: 0.744 seconds; current allocated memory: 344.939 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_3' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_3'.
INFO: [HLS 200-111]  Elapsed time: 0.504 seconds; current allocated memory: 345.889 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MaxPool2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_sitofp_dEe': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MaxPool2d'.
INFO: [HLS 200-111]  Elapsed time: 0.599 seconds; current allocated memory: 346.798 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forward_max_poo_out_2' to 'forward_max_poo_ofYi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fptrunc_64ns_32_2_1' to 'forw_back_fptruncg8j' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fpext_32ns_64_2_1' to 'forw_back_fpext_3hbi' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dadd_64ns_64ns_64_5_full_dsp_1' to 'forw_back_dadd_64ibs' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dmul_64ns_64ns_64_5_max_dsp_1' to 'forw_back_dmul_64jbC' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_ddiv_64ns_64ns_64_22_1' to 'forw_back_ddiv_64kbM' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_dexp_64ns_64ns_64_13_full_dsp_1' to 'forw_back_dexp_64lbW' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dadd_64ibs': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_ddiv_64kbM': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dexp_64lbW': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncg8j': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'forward'.
INFO: [HLS 200-111]  Elapsed time: 0.8 seconds; current allocated memory: 349.010 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MaxPooBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'MaxPooBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 1.169 seconds; current allocated memory: 349.902 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_2' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_2'.
INFO: [HLS 200-111]  Elapsed time: 0.533 seconds; current allocated memory: 350.672 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Padding' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'Padding'.
INFO: [HLS 200-111]  Elapsed time: 0.962 seconds; current allocated memory: 351.650 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d_1'.
INFO: [HLS 200-111]  Elapsed time: 1.087 seconds; current allocated memory: 352.631 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MaxPooBackPropagatio_1' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Finished creating RTL model for 'MaxPooBackPropagatio_1'.
INFO: [HLS 200-111]  Elapsed time: 0.895 seconds; current allocated memory: 353.348 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'Conv2d' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-100] Generating core module 'forw_back_fadd_32bkb': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'Conv2d'.
INFO: [HLS 200-111]  Elapsed time: 0.677 seconds; current allocated memory: 354.122 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'MatrixBackPropagatio' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'forw_back_fsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_fsub_32mb6' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fsub_32mb6': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'MatrixBackPropagatio'.
INFO: [HLS 200-111]  Elapsed time: 0.676 seconds; current allocated memory: 354.567 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'backward' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2' to 'backward_conv_grancg' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2' to 'backward_kernel_gocq' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_2_padding' to 'backward_conv_grapcA' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_2_overtu' to 'backward_kernel_gqcK' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_pool_grad_1' to 'backward_pool_grarcU' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_conv_grad_1' to 'backward_conv_grasc4' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'backward_kernel_grad_1' to 'backward_kernel_gtde' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_faddfsub_32ns_32ns_32_4_full_dsp_1' to 'forw_back_faddfsuudo' due to the length limit 20
INFO: [RTGEN 206-100] Generating core module 'forw_back_dmul_64jbC': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_faddfsuudo': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fcmp_32eOg': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fmul_32cud': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fpext_3hbi': 1 instance(s).
INFO: [RTGEN 206-100] Generating core module 'forw_back_fptruncg8j': 1 instance(s).
INFO: [RTGEN 206-100] Finished creating RTL model for 'backward'.
INFO: [HLS 200-111]  Elapsed time: 0.737 seconds; current allocated memory: 357.269 MB.
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [HLS 200-10] -- Generating RTL for module 'forw_back' 
INFO: [HLS 200-10] ----------------------------------------------------------------
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/data' to 'm_axi'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/flag' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/in_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/conv2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc1' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/fc2' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/out_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/label_r' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on port 'forw_back/lr' to 's_axilite & ap_none'.
INFO: [RTGEN 206-500] Setting interface mode on function 'forw_back' to 's_axilite & ap_ctrl_hs'.
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_1' to 'forw_back_conv_kevdy' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_conv_kernel_2' to 'forw_back_conv_kewdI' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer1' to 'forw_back_fc_hiddxdS' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_hidden_layer2' to 'forw_back_fc_hiddyd2' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_mnist_data' to 'forw_back_mnist_dzec' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_max_poo_out_1' to 'forw_back_max_pooAem' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_max_poo_locate_1' to 'forw_back_max_pooBew' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_max_poo_locate_2' to 'forw_back_max_pooCeG' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_out_1_0' to 'forw_back_fc_out_DeQ' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_fc_in_2_relu1_0' to 'forw_back_fc_in_2Ee0' due to the length limit 20
INFO: [SYN 201-210] Renamed object name 'forw_back_probability_result' to 'forw_back_probabiFfa' due to the length limit 20
INFO: [RTGEN 206-100] Bundling port 'return', 'flag', 'in_r', 'conv1', 'conv2', 'fc1', 'fc2', 'out_r', 'label_r' and 'lr' to AXI-Lite port ctrl.
INFO: [RTGEN 206-100] Finished creating RTL model for 'forw_back'.
INFO: [HLS 200-111]  Elapsed time: 2.526 seconds; current allocated memory: 360.804 MB.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_1_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_conv_out_2_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_max_poo_ofYi_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forward_fc_out_2_0_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_2_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_2_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_rgrad_1_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_wgrad_1_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_grad_0_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grancg_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_kernel_gocq_ram (RAM)' using distributed RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grapcA_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_pool_grarcU_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'backward_conv_grasc4_ram (RAM)' using block RAMs.
INFO: [RTMG 210-278] Implementing memory 'forw_back_conv_kevdy_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddxdS_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_hiddyd2_ram (RAM_2P_LUTRAM)' using distributed RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_mnist_dzec_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_max_pooAem_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_max_pooCeG_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_in_1_0_ram (RAM)' using block RAMs with power-on initialization.
INFO: [RTMG 210-278] Implementing memory 'forw_back_fc_out_DeQ_ram (RAM)' using distributed RAMs with power-on initialization.
INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:00:40 ; elapsed = 00:00:57 . Memory (MB): peak = 470.039 ; gain = 378.473
INFO: [VHDL 208-304] Generating VHDL RTL for forw_back.
INFO: [VLOG 209-307] Generating Verilog RTL for forw_back.
INFO: [HLS 200-112] Total elapsed time: 57.357 seconds; peak allocated memory: 360.804 MB.
==============================================================
Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.1 (64-bit)
Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.
==============================================================
INFO: [SYN 201-201] Setting up clock 'default' with a period of 10ns.
INFO: [HLS 200-10] Setting target device to 'xczu3eg-sbva484-1-e'
INFO: [IMPL 213-8] Exporting RTL as a Vivado IP.
